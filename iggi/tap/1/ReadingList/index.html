<!DOCTYPE HTML>
<html>
<head>
<title>JabRef references</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/javascript">
<!--
// QuickSearch script for JabRef HTML export 
// Version: 3.0
//
// Copyright (c) 2006-2011, Mark Schenk
//
// This software is distributed under a Creative Commons Attribution 3.0 License
// http://creativecommons.org/licenses/by/3.0/
//
// Features:
// - intuitive find-as-you-type searching
//    ~ case insensitive
//    ~ ignore diacritics (optional)
//
// - search with/without Regular Expressions
// - match BibTeX key
//

// Search settings
var searchAbstract = true;	// search in abstract
var searchReview = true;	// search in review

var noSquiggles = true; 	// ignore diacritics when searching
var searchRegExp = false; 	// enable RegExp searches


if (window.addEventListener) {
	window.addEventListener("load",initSearch,false); }
else if (window.attachEvent) {
	window.attachEvent("onload", initSearch); }

function initSearch() {
	// check for quick search table and searchfield
	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }

	// load all the rows and sort into arrays
	loadTableData();
	
	//find the query field
	qsfield = document.getElementById('qs_field');

	// previous search term; used for speed optimisation
	prevSearch = '';

	//find statistics location
	stats = document.getElementById('stat');
	setStatistics(-1);
	
	// set up preferences
	initPreferences();

	// shows the searchfield
	document.getElementById('quicksearch').style.display = 'block';
	document.getElementById('qs_field').onkeyup = quickSearch;
}

function loadTableData() {
	// find table and appropriate rows
	searchTable = document.getElementById('qs_table');
	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');

	// split all rows into entryRows and infoRows (e.g. abstract, review, bibtex)
	entryRows = new Array(); infoRows = new Array(); absRows = new Array(); revRows = new Array();

	// get data from each row
	entryRowsData = new Array(); absRowsData = new Array(); revRowsData = new Array(); 
	
	BibTeXKeys = new Array();
	
	for (var i=0, k=0, j=0; i<allRows.length;i++) {
		if (allRows[i].className.match(/entry/)) {
			entryRows[j] = allRows[i];
			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));
			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;
			j ++;
		} else {
			infoRows[k++] = allRows[i];
			// check for abstract/review
			if (allRows[i].className.match(/abstract/)) {
				absRows.push(allRows[i]);
				absRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			} else if (allRows[i].className.match(/review/)) {
				revRows.push(allRows[i]);
				revRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			}
		}
	}
	//number of entries and rows
	numEntries = entryRows.length;
	numInfo = infoRows.length;
	numAbs = absRows.length;
	numRev = revRows.length;
}

function quickSearch(){
	
	tInput = qsfield;

	if (tInput.value.length == 0) {
		showAll();
		setStatistics(-1);
		qsfield.className = '';
		return;
	} else {
		t = stripDiacritics(tInput.value);

		if(!searchRegExp) { t = escapeRegExp(t); }
			
		// only search for valid RegExp
		try {
			textRegExp = new RegExp(t,"i");
			closeAllInfo();
			qsfield.className = '';
		}
			catch(err) {
			prevSearch = tInput.value;
			qsfield.className = 'invalidsearch';
			return;
		}
	}
	
	// count number of hits
	var hits = 0;

	// start looping through all entry rows
	for (var i = 0; cRow = entryRows[i]; i++){

		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all
		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){
			var found = false; 

			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1){ 
				found = true;
			} else {
				if(searchAbstract && absRowsData[i]!=undefined) {
					if (absRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
				if(searchReview && revRowsData[i]!=undefined) {
					if (revRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
			}
			
			if (found){
				cRow.className = 'entry show';
				hits++;
			} else {
				cRow.className = 'entry noshow';
			}
		}
	}

	// update statistics
	setStatistics(hits)
	
	// set previous search value
	prevSearch = tInput.value;
}


// Strip Diacritics from text
// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings

// String containing replacement characters for stripping accents 
var stripstring = 
    'AAAAAAACEEEEIIII'+
    'DNOOOOO.OUUUUY..'+
    'aaaaaaaceeeeiiii'+
    'dnooooo.ouuuuy.y'+
    'AaAaAaCcCcCcCcDd'+
    'DdEeEeEeEeEeGgGg'+
    'GgGgHhHhIiIiIiIi'+
    'IiIiJjKkkLlLlLlL'+
    'lJlNnNnNnnNnOoOo'+
    'OoOoRrRrRrSsSsSs'+
    'SsTtTtTtUuUuUuUu'+
    'UuUuWwYyYZzZzZz.';

function stripDiacritics(str){

    if(noSquiggles==false){
        return str;
    }

    var answer='';
    for(var i=0;i<str.length;i++){
        var ch=str[i];
        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string
        if(chindex>=0 && chindex<stripstring.length){
            // Character is within our table, so we can strip the accent...
            var outch=stripstring.charAt(chindex);
            // ...unless it was shown as a '.'
            if(outch!='.')ch=outch;
        }
        answer+=ch;
    }
    return answer;
}

// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
// NOTE: must escape every \ in the export code because of the JabRef Export...
function escapeRegExp(str) {
  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
}

function toggleInfo(articleid,info) {

	var entry = document.getElementById(articleid);
	var abs = document.getElementById('abs_'+articleid);
	var rev = document.getElementById('rev_'+articleid);
	var bib = document.getElementById('bib_'+articleid);
	
	if (abs && info == 'abstract') {
		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract show';
	} else if (rev && info == 'review') {
		rev.className.indexOf('noshow') == -1?rev.className = 'review noshow':rev.className = 'review show';
	} else if (bib && info == 'bibtex') {
		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex show';
	} else { 
		return;
	}

	// check if one or the other is available
	var revshow; var absshow; var bibshow;
	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;
	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;	
	(bib && bib.className.indexOf('noshow') == -1)? bibshow = true: bibshow = false;
	
	// highlight original entry
	if(entry) {
		if (revshow || absshow || bibshow) {
		entry.className = 'entry highlight show';
		} else {
		entry.className = 'entry show';
		}
	}
	
	// When there's a combination of abstract/review/bibtex showing, need to add class for correct styling
	if(absshow) {
		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';
	} 
	if (revshow) {
		bibshow?rev.className = 'review nextshow': rev.className = 'review';
	}	
	
}

function setStatistics (hits) {
	if(hits < 0) { hits=numEntries; }
	if(stats) { stats.firstChild.data = hits + '/' + numEntries}
}

function getTextContent(node) {
	// Function written by Arve Bersvendsen
	// http://www.virtuelvis.com
	
	if (node.nodeType == 3) {
	return node.nodeValue;
	} // text node
	if (node.nodeType == 1 && node.className != "infolinks") { // element node
	var text = [];
	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {
		text.push(getTextContent(chld));
	}
	return text.join("");
	} return ""; // some other node, won't contain text nodes.
}

function showAll(){
	closeAllInfo();
	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }
}

function closeAllInfo(){
	for (var i=0; i < numInfo; i++){
		if (infoRows[i].className.indexOf('noshow') ==-1) {
			infoRows[i].className = infoRows[i].className + ' noshow';
		}
	}
}

function clearQS() {
	qsfield.value = '';
	showAll();
}

function redoQS(){
	showAll();
	quickSearch(qsfield);
}

function updateSetting(obj){
	var option = obj.id;
	var checked = obj.value;

	switch(option)
	 {
	 case "opt_searchAbs":
	   searchAbstract=!searchAbstract;
	   redoQS();
	   break;
	 case "opt_searchRev":
	   searchReview=!searchReview;
	   redoQS();
	   break;
	 case "opt_useRegExp":
	   searchRegExp=!searchRegExp;
	   redoQS();
	   break;
	 case "opt_noAccents":
	   noSquiggles=!noSquiggles;
	   loadTableData();
	   redoQS();
	   break;
	 }
}

function initPreferences(){
	if(searchAbstract){document.getElementById("opt_searchAbs").checked = true;}
	if(searchReview){document.getElementById("opt_searchRev").checked = true;}
	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}
	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}
	
	if(numAbs==0) {document.getElementById("opt_searchAbs").parentNode.style.display = 'none';}
	if(numRev==0) {document.getElementById("opt_searchRev").parentNode.style.display = 'none';}
}

function toggleSettings(){
	var togglebutton = document.getElementById('showsettings');
	var settings = document.getElementById('settings');
	
	if(settings.className == "hidden"){
		settings.className = "show";
		togglebutton.innerText = "close settings";
		togglebutton.textContent = "close settings";
	}else{
		settings.className = "hidden";
		togglebutton.innerText = "settings...";		
		togglebutton.textContent = "settings...";
	}
}

-->
</script>
<style type="text/css">
body { background-color: white; font-family: Arial, sans-serif; font-size: 13px; line-height: 1.2; padding: 1em; color: #2E2E2E; margin: auto 2em; }

form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }
span#searchstat {padding-left: 1em;}

div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }
div#settings ul {margin: 0; padding: 0; }
div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }
div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}
div#settings input { margin-bottom: 0px;}

div#settings.hidden {display:none;}

#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }
#showsettings:hover { cursor: pointer; }

.invalidsearch { background-color: red; }
input[type="button"] { background-color: #efefef; border: 1px #2E2E2E solid;}

table { width: 100%; empty-cells: show; border-spacing: 0em 0.2em; margin: 1em 0em; border-style: none; }
th, td { border: 1px gray solid; border-width: 1px 1px; padding: 0.5em; vertical-align: top; text-align: left; }
th { background-color: #efefef; }
td + td, th + th { border-left: none; }

td a { color: navy; text-decoration: none; }
td a:hover  { text-decoration: underline; }

tr.noshow { display: none;}
tr.highlight td { background-color: #EFEFEF; border-top: 2px #2E2E2E solid; font-weight: bold; }
tr.abstract td, tr.review td, tr.bibtex td { background-color: #EFEFEF; text-align: justify; border-bottom: 2px #2E2E2E solid; }
tr.nextshow td { border-bottom: 1px gray solid; }

tr.bibtex pre { width: 100%; overflow: auto; white-space: pre-wrap;}
p.infolinks { margin: 0.3em 0em 0em 0em; padding: 0px; }

@media print {
	p.infolinks, #qs_settings, #quicksearch, t.bibtex { display: none !important; }
	tr { page-break-inside: avoid; }
}
</style>
</head>
<body>

<form action="" id="quicksearch">
<input type="text" id="qs_field" autocomplete="off" placeholder="Type to search..." /> <input type="button" onclick="clearQS()" value="clear" />
<span id="searchstat">Matching entries: <span id="stat">0</span></span>
<div id="showsettings" onclick="toggleSettings()">settings...</div>
<div id="settings" class="hidden">
<ul>
<li><input type="checkbox" class="search_setting" id="opt_searchAbs" onchange="updateSetting(this)"><label for="opt_searchAbs"> include abstract</label></li>
<li><input type="checkbox" class="search_setting" id="opt_searchRev" onchange="updateSetting(this)"><label for="opt_searchRev"> include review</label></li>
<li><input type="checkbox" class="search_setting" id="opt_useRegExp" onchange="updateSetting(this)"><label for="opt_useRegExp"> use RegExp</label></li>
<li><input type="checkbox" class="search_setting" id="opt_noAccents" onchange="updateSetting(this)"><label for="opt_noAccents"> ignore accents</label></li>
</ul>
</div>
</form>

<p>All papers have been read, but some papers (particularly those on Relational Reinforcement Learning, Information Set Monte Carlo Tree Search, and papers that require significant effort to understand fully due to dependencies on other works) will require further reading to be fully understood.</p>

<p>There may be some slight formatting errors in how the papers or abstracts are listed. I haven't had the chance to tidy up my .bib file, yet!</p>

<table id="qs_table" border="1">
<thead><tr><th width="20%">Author</th><th width="35%">Title</th><th width="5%">Year</th><th width="30%">Journal/Proceedings</th><th width="10%">Reftype</th></tr></thead>
<tbody><tr id="Simsek2004Using" class="entry">
	<td>&#350;im&#351;ek, &Ouml;. and Barto, A.G.</td>
	<td>Using relative novelty to identify useful temporal abstractions in reinforcement learning <p class="infolinks">[<a href="javascript:toggleInfo('Simsek2004Using','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Simsek2004Using','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td>Proc. IMLS International Conference on Machine Learning, pp. 95&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Simsek2004Using" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a new method for automatically creating useful temporal abstractions in reinforcement learning. We argue that states that allow the agent to transition to a different region of the state space are useful subgoals, and propose a method for identifying them using the concept of relative novelty. When such a state is identified, a temporally-extended activity (e.g., an option) is generated that takes the agent efficiently to this state. We illustrate the utility of the method in a number of tasks.</td>
</tr>
<tr id="bib_Simsek2004Using" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Simsek2004Using,
  author = {&#350;im&#351;ek, &Ouml;zg&uuml;r and Barto, Andrew G},
  title = {Using relative novelty to identify useful temporal abstractions in reinforcement learning},
  booktitle = {Proc. IMLS International Conference on Machine Learning},
  year = {2004},
  pages = {95}
}
</pre></td>
</tr>
<tr id="Abbasnejad2015Loss" class="entry">
	<td>Abbasnejad, E., Domke, J. and Sanner, S.</td>
	<td>Loss-calibrated Monte Carlo Action Selection <p class="infolinks">[<a href="javascript:toggleInfo('Abbasnejad2015Loss','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Abbasnejad2015Loss','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proc. AAAI Conference on Artificial Intelligence&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Abbasnejad2015Loss" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Bayesian decision-theory underpins robust decision making in applications ranging from plant control to robotics where hedging action selection against state uncertainty is critical for minimizing low probability but potentially catastrophic outcomes (e.g, uncontrollable plant conditions or robots falling into stairwells). Unfortunately, belief state distributions in such settings are often complex and/or high dimensional, thus prohibiting the efficient application of analytical techniques for expected utility computation when real-time control is required. This leaves Monte Carlo evaluation as one of the few viable (and hence frequently used) techniques for online action selection. However, loss-insensitive Monte Carlo methods may require large numbers of samples to identify optimal actions with high certainty since they may sample from high probability regions that do not disambiguate action utilities. In this paper we remedy this problem by deriving an optimal proposal distribution for a loss-calibrated Monte Carlo importance sampler that bounds the regret of using an estimated optimal action. Empirically, we show that using our loss-calibrated Monte Carlo method yields high-accuracy optimal action selections in a fraction of the number of samples required by conventional loss-insensitive samplers.</td>
</tr>
<tr id="bib_Abbasnejad2015Loss" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Abbasnejad2015Loss,
  author = {Abbasnejad, Ehsan and Domke, Justin and Sanner, Scott},
  title = {Loss-calibrated Monte Carlo Action Selection},
  booktitle = {Proc. AAAI Conference on Artificial Intelligence},
  year = {2015}
}
</pre></td>
</tr>
<tr id="Alhejali2013Using" class="entry">
	<td>Alhejali, A. and Lucas, S.</td>
	<td>Using Genetic Programming to Evolve Heuristics for a Monte Carlo Tree Search Ms Pac-Man Agent <p class="infolinks">[<a href="javascript:toggleInfo('Alhejali2013Using','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Alhejali2013Using','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Proc. IEEE Conference on Computational Intelligence in Games, pp. 1-8&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Alhejali2013Using" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Ms Pac-Man is one of the most challenging test beds in game artificial intelligence (AI). Genetic programming and Monte Carlo Tree Search (MCTS) have already been successful applied to several games including Pac-Man. In this paper, we use Monte Carlo Tree Search to create a Ms Pac-Man playing agent before using genetic programming to enhance its performance by evolving a new default policy to replace the random agent used in the simulations. The new agent with the evolved default policy was able to achieve an 18% increase on its average score over the agent with random default policy.</td>
</tr>
<tr id="bib_Alhejali2013Using" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Alhejali2013Using,
  author = {Alhejali, AM. and Lucas, S.M.},
  title = {Using Genetic Programming to Evolve Heuristics for a Monte Carlo Tree Search Ms Pac-Man Agent},
  booktitle = {Proc. IEEE Conference on Computational Intelligence in Games},
  year = {2013},
  pages = {1-8}
}
</pre></td>
</tr>
<tr id="Allis1994Searching" class="entry">
	<td>Allis, L.V.</td>
	<td>Searching for solutions in games and artificial intelligence <p class="infolinks" [<a href="javascript:toggleInfo('Allis1994Searching','bibtex')">BibTeX</a>]</p></td>
	<td>1994</td>
	<td>&nbsp;</td>
	<td>book</td>
</tr>
<tr id="bib_Allis1994Searching" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@book{Allis1994Searching,
  author = {Allis, Louis Victor},
  title = {Searching for solutions in games and artificial intelligence},
  publisher = {Ponsen &amp; Looijen},
  year = {1994}
}
</pre></td>
</tr>
<tr id="Amato2010High" class="entry">
	<td>Amato, C. and Shani, G.</td>
	<td>High-level Reinforcement Learning in Strategy Games <p class="infolinks">[<a href="javascript:toggleInfo('Amato2010High','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Amato2010High','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>Vol. 1Proc. International Conference on Autonomous Agents and Multiagent Systems, pp. 75-82&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Amato2010High" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Video games provide a rich testbed for artificial intelligence methods. In particular, creating automated opponents that perform well in strategy games is a difficult task. For instance, human players rapidly discover and exploit the weaknesses of hard coded strategies. To build better strategies, we suggest a reinforcement learning approach for learning a policy that switches between high-level strategies. These strategies are chosen based on different game situations and a fixed opponent strategy. Our learning agents are able to rapidly adapt to fixed opponents and improve deficiencies in the hard coded strategies, as the results demonstrate.</td>
</tr>
<tr id="bib_Amato2010High" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Amato2010High,
  author = {Amato, Christopher and Shani, Guy},
  title = {High-level Reinforcement Learning in Strategy Games},
  booktitle = {Proc. International Conference on Autonomous Agents and Multiagent Systems},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  year = {2010},
  volume = {1},
  pages = {75--82}
}
</pre></td>
</tr>
<tr id="Andre2002State" class="entry">
	<td>Andre, D. and Russell, S.J.</td>
	<td>State abstraction for programmable reinforcement learning agents <p class="infolinks">[<a href="javascript:toggleInfo('Andre2002State','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Andre2002State','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td>Proc. AAAI Conference on Artificial Intelligence, pp. 119-125&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Andre2002State" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Safe state abstraction in reinforcement learning allows an agent to ignore aspects of its current state that are irrelevant to its current decision, and therefore speeds up dynamic programming and learning. This paper explores safe state abstraction in hierarchical reinforcement learning, where learned behaviors must conform to a given partial, hierarchical program. Unlike previous approaches to this problem, our methods yield significant state abstraction while maintaining hierarchical optimality, i.e., optimality among all policies consistent with the partial program. We show how to achieve this for a partial programming language that is essentially Lisp augmented with nondeterministic constructs. We demonstrate our methods on two variants of Dietterich’s taxi domain, showing how state abstraction and hierarchical optimality result in faster learning of better policies and enable the transfer of learned skills from one problem to another.</td>
</tr>
<tr id="bib_Andre2002State" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Andre2002State,
  author = {Andre, David and Russell, Stuart J},
  title = {State abstraction for programmable reinforcement learning agents},
  booktitle = {Proc. AAAI Conference on Artificial Intelligence},
  year = {2002},
  pages = {119--125}
}
</pre></td>
</tr>
<tr id="Asadpour2006Reduction" class="entry">
	<td>Asadpour, M., Ahmadabadi, M.N. and Siegwart, R.</td>
	<td>Reduction of learning time for robots using automatic state abstraction <p class="infolinks">[<a href="javascript:toggleInfo('Asadpour2006Reduction','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Asadpour2006Reduction','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>Proc. European Robotics Symposium, pp. 79-92&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Asadpour2006Reduction" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The required learning time and curse of dimensionality restrict the applicability of Reinforcement Learning(RL) on real robots. Difficulty in inclusion of initial knowledge and understanding the learned rules must be added to the mentioned problems. In this paper we address automatic state abstraction and creation of hierarchies in RL agent’s mind, as two major approaches for reducing the number of learning trials, simplifying inclusion of prior knowledge, and making the learned rules more abstract and understandable. We formalize automatic state abstraction and hierarchy creation as an optimization problem and derive a new algorithm that adapts decision tree learning techniques to state abstraction. The proof of performance is supported by strong evidences from simulation results in nondeterministic environments. Simulation results show encouraging enhancements in the required number of learning trials, agent’s performance, size of the learned trees, and computation time of the algorithm.</td>
</tr>
<tr id="bib_Asadpour2006Reduction" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Asadpour2006Reduction,
  author = {Asadpour, Masoud and Ahmadabadi, Majid Nili and Siegwart, Roland},
  title = {Reduction of learning time for robots using automatic state abstraction},
  booktitle = {Proc. European Robotics Symposium},
  year = {2006},
  pages = {79--92}
}
</pre></td>
</tr>
<tr id="Asmuth2011Approaching" class="entry">
	<td>Asmuth, J. and Littman, M.L.</td>
	<td>Approaching Bayes-optimalilty using Monte-Carlo tree search <p class="infolinks">[<a href="javascript:toggleInfo('Asmuth2011Approaching','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Asmuth2011Approaching','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Proc. International Conference on Automated Planning and Sceduling&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Asmuth2011Approaching" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Bayes-optimal behavior, while well-defined, is often difficult to achieve. Recent advances in the use of Monte-Carlo tree search (MCTS) have shown that it is possible to act nearoptimally in Markov Decision Processes (MDPs) with very large or infinite state spaces. Bayes-optimal behavior in an unknown MDP is equivalent to optimal behavior in the known belief-space MDP, although the size of this belief-space MDP grows exponentially with the amount of history retained, and is potentially infinite. We show how an agent can use one particular MCTS algorithm, Forward Search Sparse Sampling (FSSS), in an efficient way to approach Bayes-optimality.</td>
</tr>
<tr id="bib_Asmuth2011Approaching" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Asmuth2011Approaching,
  author = {Asmuth, John and Littman, Michael L},
  title = {Approaching Bayes-optimalilty using Monte-Carlo tree search},
  booktitle = {Proc. International Conference on Automated Planning and Sceduling},
  year = {2011}
}
</pre></td>
</tr>
<tr id="Auer2002Finite" class="entry">
	<td>Auer, P., Cesa-Bianchi, N. and Fischer, P.</td>
	<td>Finite-time analysis of the multiarmed bandit problem <p class="infolinks">[<a href="javascript:toggleInfo('Auer2002Finite','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Auer2002Finite','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td>Machine learning<br/>Vol. 47(2-3), pp. 235-256&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Auer2002Finite" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy’s success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.</td>
</tr>
<tr id="bib_Auer2002Finite" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Auer2002Finite,
  author = {Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  title = {Finite-time analysis of the multiarmed bandit problem},
  journal = {Machine learning},
  publisher = {Springer},
  year = {2002},
  volume = {47},
  number = {2-3},
  pages = {235--256}
}
</pre></td>
</tr>
<tr id="Barto1998Reinforcement" class="entry">
	<td>Barto, A.G.</td>
	<td>Reinforcement Learning: An Introduction <p class="infolinks">[<a href="javascript:toggleInfo('Barto1998Reinforcement','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Barto1998Reinforcement','bibtex')">BibTeX</a>]</p></td>
	<td>1998</td>
	<td>&nbsp;</td>
	<td>book</td>
</tr>
<tr id="abs_Barto1998Reinforcement" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement learning, one of the most active research areas in artificialintelligence, is a computational approach to learning whereby an agent tries to maximize the totalamount of reward it receives when interacting with a complex, uncertain environment. InReinforcement Learning, Richard Sutton and Andrew Barto provide a clear andsimple account of the key ideas and algorithms of reinforcement learning. Their discussion rangesfrom the history of the field's intellectual foundations to the most recent developments andapplications. The only necessary mathematical background is familiarity with elementary concepts ofprobability.<p>The book is divided into three parts. Part I defines thereinforcement learning problem in terms of Markov decision processes. Part II provides basicsolution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. PartIII presents a unified view of the solution methods and incorporates artificial neural networks,eligibility traces, and planning; the two final chapters present case studies and consider thefuture of reinforcement learning.</td>
</tr>
<tr id="bib_Barto1998Reinforcement" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@book{Barto1998Reinforcement,
  author = {Barto, Andrew G},
  title = {Reinforcement Learning: An Introduction},
  publisher = {MIT Press},
  year = {1998}
}
</pre></td>
</tr>
<tr id="Barto2003Recent" class="entry">
	<td>Barto, A.G. and Mahadevan, S.</td>
	<td>Recent Advances in Hierarchical Reinforcement Learning <p class="infolinks">[<a href="javascript:toggleInfo('Barto2003Recent','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Barto2003Recent','bibtex')">BibTeX</a>]</p></td>
	<td>2003</td>
	<td>Discrete Event Dynamic Systems<br/>Vol. 13(4), pp. 341-379&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Barto2003Recent" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.</td>
</tr>
<tr id="bib_Barto2003Recent" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Barto2003Recent,
  author = {Barto, Andrew G and Mahadevan, Sridhar},
  title = {Recent Advances in Hierarchical Reinforcement Learning},
  journal = {Discrete Event Dynamic Systems},
  publisher = {Springer},
  year = {2003},
  volume = {13},
  number = {4},
  pages = {341--379}
}
</pre></td>
</tr>
<tr id="Bedder2014Plan" class="entry">
	<td>Bedder, M.</td>
	<td>Plan-Based Monte Carlo Tree Search <p class="infolinks">[<a href="javascript:toggleInfo('Bedder2014Plan','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Bedder2014Plan','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td><i>School</i>: University of York, Department of Computer Science&nbsp;</td>
	<td>mastersthesis</td>
</tr>
<tr id="abs_Bedder2014Plan" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this project I have propose Plan-Based Monte Carlo Tree Search, a modification of Monte Carlo Tree Search that utilises information gained for abstractions in order to guide searching over diffcult state spaces, using techniques taken from Reinforcement Learning approaches. In this, I aim to generate a searching algorithm that is effective for hugely-branching games without the usage of explicit heuristic functions.<p>This project demonstrates that searching over abstractions of game states can provide a method for augmenting existing search techniques, with my method achieving performance similar to that of Monte Carlo Tree Search using the UCT algorithm. I also propose further steps that can be taken in the development of this technique in order to increase the performance over different problem domains.</td>
</tr>
<tr id="bib_Bedder2014Plan" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@mastersthesis{Bedder2014Plan,
  author = {Bedder, Matthew},
  title = {Plan-Based Monte Carlo Tree Search},
  school = {University of York, Department of Computer Science},
  year = {2014}
}
</pre></td>
</tr>
<tr id="Benbassat2013EvoMCTS" class="entry">
	<td>Benbassat, A. and Sipper, M.</td>
	<td>EvoMCTS: Enhancing MCTS-based players through genetic programming <p class="infolinks">[<a href="javascript:toggleInfo('Benbassat2013EvoMCTS','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Benbassat2013EvoMCTS','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Proc. IEEE Conference on Computational Intelligence in Games, pp. 1-8&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Benbassat2013EvoMCTS" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present EvoMCTS, a genetic programming method for enhancing level of play in games. Our work focuses on the zero-sum, deterministic, perfect-information board game of Reversi. Expanding on our previous work on evolving board-state evaluation functions for alpha-beta search algorithm variants, we now evolve evaluation functions that augment the MTCS algorithm. We use strongly typed genetic programming, explicitly defined introns, and a selective directional crossover method. Our system regularly evolves players that outperform MCTS players that use the same amount of search. Our results prove scalable and EvoMCTS players whose search is increased offline still outperform MCTS counterparts. To demonstrate the generality of our method we apply EvoMCTS successfully to the game of Dodgem.</td>
</tr>
<tr id="bib_Benbassat2013EvoMCTS" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Benbassat2013EvoMCTS,
  author = {Benbassat, A and Sipper, M.},
  title = {EvoMCTS: Enhancing MCTS-based players through genetic programming},
  booktitle = {Proc. IEEE Conference on Computational Intelligence in Games},
  year = {2013},
  pages = {1-8}
}
</pre></td>
</tr>
<tr id="Bouzy2004Monte" class="entry">
	<td>Bouzy, B. and Helmstetter, B.</td>
	<td>Monte-Carlo Go Developments <p class="infolinks">[<a href="javascript:toggleInfo('Bouzy2004Monte','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Bouzy2004Monte','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td>Advances in Computer Games, pp. 159-174&nbsp;</td>
	<td>incollection</td>
</tr>
<tr id="abs_Bouzy2004Monte" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We describe two Go programs, Olga and Oleg, developed by a Monte-Carlo approach that is simpler than Bruegmann’s (1993) approach. Our method is based on Abramson (1990). We performed experiments, to assess ideas on (1) progressive pruning, (2) all moves as first heuristic, (3) temperature, (4) simulated annealing, and (5) depth-two tree search within the Monte-Carlo framework. Progressive pruning and the all moves as first heuristic are good speed-up enhancements that do not deteriorate the level of the program too much. Then, using a constant temperature is an adequate and simple heuristic that is about as good as simulated annealing. The depth-two heuristic gives deceptive results at the moment. The results of our Monte-Carlo programs against knowledge-based programs on 9x9 boards are promising. Finally, the ever-increasing power of computers lead us to think that Monte-Carlo approaches are worth considering for computer Go in the future.</td>
</tr>
<tr id="bib_Bouzy2004Monte" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Bouzy2004Monte,
  author = {Bouzy, Bruno and Helmstetter, Bernard},
  title = {Monte-Carlo Go Developments},
  booktitle = {Advances in Computer Games},
  publisher = {Springer},
  year = {2004},
  pages = {159--174}
}
</pre></td>
</tr>
<tr id="Branavan2011Learning" class="entry">
	<td>Branavan, S., Silver, D. and Barzilay, R.</td>
	<td>Learning to Win by Reading Manuals in a Monte-Carlo Framework <p class="infolinks">[<a href="javascript:toggleInfo('Branavan2011Learning','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Branavan2011Learning','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td><br/>Vol. 1Proc. Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 268-277&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Branavan2011Learning" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games. Our ultimate goal is to enrich a stochastic player with high-level guidance expressed in text. Our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text. Our method operates in the Monte-Carlo search framework, and learns both text analysis and game strategies based only on environment feedback. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 27% absolute improvement and winning over 78% of games when playing against the built-in AI of Civilization II.</td>
</tr>
<tr id="bib_Branavan2011Learning" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Branavan2011Learning,
  author = {Branavan, SRK and Silver, David and Barzilay, Regina},
  title = {Learning to Win by Reading Manuals in a Monte-Carlo Framework},
  booktitle = {Proc. Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  year = {2011},
  volume = {1},
  pages = {268--277}
}
</pre></td>
</tr>
<tr id="Branavan2011Non" class="entry">
	<td>Branavan, S., Silver, D. and Barzilay, R.</td>
	<td>Non-Linear Monte-Carlo Search in Civilization II <p class="infolinks">[<a href="javascript:toggleInfo('Branavan2011Non','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Branavan2011Non','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td><br/>Vol. 3Proc. International Joint Conference on Artificial Intelligence, pp. 2404-2410&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Branavan2011Non" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a new Monte-Carlo search algorithm for very large sequential decision-making problems. Our approach builds on the recent success of Monte-Carlo tree search algorithms, which estimate the value of states and actions from the mean outcome of random simulations. Instead of using a search tree, we apply non-linear regression, online, to estimate a state-action value function from the outcomes of random simulations. This value function generalizes between related states and actions, and can therefore provide more accurate evaluations after fewer simulations. We apply our Monte-Carlo search algorithm to the game of Civilization II, a challenging multi-agent strategy game with an enormous state space and around $10^21$ joint actions. We approximate the value function by a neural network, augmented by linguistic knowledge that is extracted automatically from the official game manual. We show that this non-linear value function is significantly more efficient than a linear value function. Our non-linear Monte-Carlo search wins 80% of games against the handcrafted, built-in AI for Civilization II.</td>
</tr>
<tr id="bib_Branavan2011Non" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Branavan2011Non,
  author = {Branavan, SRK and Silver, David and Barzilay, Regina},
  title = {Non-Linear Monte-Carlo Search in Civilization II},
  booktitle = {Proc. International Joint Conference on Artificial Intelligence},
  publisher = {AAAI Press},
  year = {2011},
  volume = {3},
  pages = {2404--2410}
}
</pre></td>
</tr>
<tr id="Brooks1986Achieving" class="entry">
	<td>Brooks, R.A.</td>
	<td>Achieving artificial intelligence through building robots <p class="infolinks">[<a href="javascript:toggleInfo('Brooks1986Achieving','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Brooks1986Achieving','bibtex')">BibTeX</a>]</p></td>
	<td>1986</td>
	<td>&nbsp;</td>
	<td>techreport</td>
</tr>
<tr id="abs_Brooks1986Achieving" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We argue that generally accepted methodologies of Artificial Intelligence research are limited in the proportion of human level intelligence they can be expected to emulate. We argue that the currently accepted decompositions and static representations used in such research are wrong. We argue for a shift to a process based model, with a decomposition based on task achieving behaviors as the organizational principle. In particular we advocate building robotic insects.</td>
</tr>
<tr id="bib_Brooks1986Achieving" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@techreport{Brooks1986Achieving,
  author = {Brooks, Rodney A},
  title = {Achieving artificial intelligence through building robots},
  year = {1986}
}
</pre></td>
</tr>
<tr id="Browne2012survey" class="entry">
	<td>Browne, C.B., Powley, E., Whitehouse, D., Lucas, S.M., Cowling, P.I., Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S. and Colton, S.</td>
	<td>A Survey of Monte Carlo Tree Search Methods <p class="infolinks">[<a href="javascript:toggleInfo('Browne2012survey','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Browne2012survey','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>IEEE Trans. on Computational Intelligence and AI in Games (CAIG'12)<br/>Vol. 4(1), pp. 1-43&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Browne2012survey" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Monte Carlo Tree Search (MCTS) is a recently proposed search method that combines the precision of tree search with the<br>generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer<br>Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a<br>snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm’s derivation, impart some<br>structure on the many variations and enhancements that have been proposed, and summarise the results from the key game and<br>non-game domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for<br>future work.</td>
</tr>
<tr id="bib_Browne2012survey" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Browne2012survey,
  author = {Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter I and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  title = {A Survey of Monte Carlo Tree Search Methods},
  journal = {IEEE Trans. on Computational Intelligence and AI in Games (CAIG'12)},
  publisher = {IEEE},
  year = {2012},
  volume = {4},
  number = {1},
  pages = {1--43}
}
</pre></td>
</tr>
<tr id="Bulitko2007Graph" class="entry">
	<td>Bulitko, V., Sturtevant, N.R., Lu, J. and Yau, T.</td>
	<td>Graph Abstraction in Real-time Heuristic Search. <p class="infolinks">[<a href="javascript:toggleInfo('Bulitko2007Graph','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Bulitko2007Graph','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>Journal of Artificial Intelligence Research<br/>Vol. 30, pp. 51-100&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Bulitko2007Graph" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Real-time heuristic search methods are used by situated agents in applications that require the amount of planning per move to be independent of the problem size. Such agents plan only a few actions at a time in a local search space and avoid getting trapped in local minima by improving their heuristic function over time. We extend a wide class of real-time search algorithms with automatically-built state abstraction and prove completeness and convergence of the resulting family of algorithms. We then analyze the impact of abstraction in an extensive empirical study in real-time pathfinding. Abstraction is found to improve efficiency by providing better trading offs between planning time, learning speed and other negatively correlated performance measures.</td>
</tr>
<tr id="bib_Bulitko2007Graph" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Bulitko2007Graph,
  author = {Bulitko, Vadim and Sturtevant, Nathan R and Lu, Jieshan and Yau, Timothy},
  title = {Graph Abstraction in Real-time Heuristic Search.},
  journal = {Journal of Artificial Intelligence Research},
  year = {2007},
  volume = {30},
  pages = {51--100}
}
</pre></td>
</tr>
<tr id="Cazenave2009Nested" class="entry">
	<td>Cazenave, T.</td>
	<td>Nested Monte-Carlo Search. <p class="infolinks">[<a href="javascript:toggleInfo('Cazenave2009Nested','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Cazenave2009Nested','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td><br/>Vol. 9Proc. International Joint Conference on Artificial Intelligence, pp. 456-461&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Cazenave2009Nested" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Many problems have a huge state space and no good heuristic to order moves so as to guide the search toward the best positions. Random games can be used to score positions and evaluate their interest. Random games can also be improved using random games to choose a move to try at each step of a game. Nested Monte-Carlo Search addresses the problem of guiding the search toward better states when there is no available heuristic. It uses nested levels of random games in order to guide the search. The algorithm is studied theoretically on simple abstract problems and applied successfully to three different games: Morpion Solitaire, SameGame and 16x16 Sudoku.</td>
</tr>
<tr id="bib_Cazenave2009Nested" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Cazenave2009Nested,
  author = {Cazenave, Tristan},
  title = {Nested Monte-Carlo Search.},
  booktitle = {Proc. International Joint Conference on Artificial Intelligence},
  year = {2009},
  volume = {9},
  pages = {456--461}
}
</pre></td>
</tr>
<tr id="Chaslot2010Monte" class="entry">
	<td>Chaslot, G.</td>
	<td>Monte-Carlo Tree Search <p class="infolinks" [<a href="javascript:toggleInfo('Chaslot2010Monte','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td><i>School</i>: Universiteit Maastricht&nbsp;</td>
	<td>phdthesis</td>
</tr>
<tr id="bib_Chaslot2010Monte" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@phdthesis{Chaslot2010Monte,
  author = {Chaslot, Guillaume},
  title = {Monte-Carlo Tree Search},
  school = {Universiteit Maastricht},
  year = {2010}
}
</pre></td>
</tr>
<tr id="Chaslot2007Progressive" class="entry">
	<td>Chaslot, G., Winands, M., Uiterwijk, J., van den Herik, H. and Bouzy, B.</td>
	<td>Progressive strategies for Monte-Carlo Tree Search <p class="infolinks">[<a href="javascript:toggleInfo('Chaslot2007Progressive','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chaslot2007Progressive','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>Proc. 10th Joint Conference on Information Sciences, pp. 655-661&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Chaslot2007Progressive" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Monte-Carlo Tree Search (MCTS) is a new best-first search guided by the results of Monte-Carlo simulations. In this article we introduce two progressive strategies for MCTS, called progressive bias and progressive unpruning. They enable the use of relatively time-expensive heuristic knowledge without speed reduction. Progressive bias directs the search according to heuristic knowledge. Progressive unpruning first reduces the branching factor, and then increases it gradually again. Experiments assess that the two progressive strategies significantly improve the level of our Go program Mango. Moreover, we see that the combination of both strategies performs even better on larger board sizes.</td>
</tr>
<tr id="bib_Chaslot2007Progressive" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Chaslot2007Progressive,
  author = {Chaslot, GMJB and Winands, Mark and Uiterwijk, JWHM and van den Herik, H and Bouzy, Bruno},
  title = {Progressive strategies for Monte-Carlo Tree Search},
  booktitle = {Proc. 10th Joint Conference on Information Sciences},
  year = {2007},
  pages = {655--661}
}
</pre></td>
</tr>
<tr id="Chung2005Monte" class="entry">
	<td>Chung, M., Buro, M. and Schaeffer, J.</td>
	<td>Monte Carlo planning in RTS games <p class="infolinks">[<a href="javascript:toggleInfo('Chung2005Monte','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chung2005Monte','bibtex')">BibTeX</a>]</p></td>
	<td>2005</td>
	<td>Proc. IEEE Conference on Computational Intelligence in Games&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Chung2005Monte" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Monte Carlo simulations have been successfully used in classic turn–based games such as backgammon, bridge, poker, and Scrabble. In this paper, we apply the ideas to the problem of planning in games with imperfect information, stochasticity, and simultaneous moves. The domain we consider is real–time strategy games. We present a framework — MCPlan — for Monte Carlo planning, identify its performance parameters, and analyze the results of an implementation in a capture–the–flag game.</td>
</tr>
<tr id="bib_Chung2005Monte" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Chung2005Monte,
  author = {Chung, Michael and Buro, Michael and Schaeffer, Jonathan},
  title = {Monte Carlo planning in RTS games},
  booktitle = {Proc. IEEE Conference on Computational Intelligence in Games},
  year = {2005}
}
</pre></td>
</tr>
<tr id="Ciancarini2009Monte" class="entry">
	<td>Ciancarini, P. and Favini, G.P.</td>
	<td>Monte Carlo Tree Search Techniques in the Game of Kriegspiel. <p class="infolinks">[<a href="javascript:toggleInfo('Ciancarini2009Monte','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ciancarini2009Monte','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td><br/>Vol. 9Proc. International Joint Conference on Artificial Intelligence, pp. 474-479&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Ciancarini2009Monte" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Monte Carlo tree search has brought significant improvements to the level of computer players in games such as Go, but so far it has not been used very extensively in games of strongly imperfect information with a dynamic board and an emphasis on risk management and decision making under uncertainty. In this paper we explore its application to the game of Kriegspiel (invisible chess), providing three Monte Carlo methods of increasing strength for playing the game with little specific knowledge. We compare these Monte Carlo agents to the strongest known minimax-based Kriegspiel player, obtaining significantly better results with a considerably simpler logic and less domain-specific knowledge.</td>
</tr>
<tr id="bib_Ciancarini2009Monte" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Ciancarini2009Monte,
  author = {Ciancarini, Paolo and Favini, Gian Piero},
  title = {Monte Carlo Tree Search Techniques in the Game of Kriegspiel.},
  booktitle = {Proc. International Joint Conference on Artificial Intelligence},
  year = {2009},
  volume = {9},
  pages = {474--479}
}
</pre></td>
</tr>
<tr id="Coulom2007Efficient" class="entry">
	<td>Coulom, R&eacute;.</td>
	<td>Efficient selectivity and backup operators in Monte-Carlo tree search <p class="infolinks">[<a href="javascript:toggleInfo('Coulom2007Efficient','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Coulom2007Efficient','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>Computers and games, pp. 72-83&nbsp;</td>
	<td>incollection</td>
</tr>
<tr id="abs_Coulom2007Efficient" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9×9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.</td>
</tr>
<tr id="bib_Coulom2007Efficient" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Coulom2007Efficient,
  author = {Coulom, R&eacute;mi},
  title = {Efficient selectivity and backup operators in Monte-Carlo tree search},
  booktitle = {Computers and games},
  publisher = {Springer},
  year = {2007},
  pages = {72--83}
}
</pre></td>
</tr>
<tr id="Cowling2013Search" class="entry">
	<td>Cowling, P.I., Buro, M., Bida, M., Botea, A., Bouzy, B., Butz, M.V., Hingston, P., Mu&ntilde;oz-Avila, H., Nau, D. and Sipper, M.</td>
	<td>Search in Real-Time Video Games <p class="infolinks">[<a href="javascript:toggleInfo('Cowling2013Search','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Cowling2013Search','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td><br/>Vol. 6Artificial and Computational Intelligence in Games, pp. 1-19&nbsp;</td>
	<td>incollection</td>
</tr>
<tr id="abs_Cowling2013Search" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This chapter arises from the discussions of an experienced international group of researchers interested in the potential for creative application of algorithms for searching finite discrete graphs, which have been highly successful in a wide range of application areas, to address a broad range of problems arising in video games. The chapter first summarises the state of the art in search algorithms for games. It then considers the challenges in implementing these algorithms in video games (particularly real time strategy and first-person games) and ways of creating searchable discrete representations of video game decisions (for example as state-action graphs). Finally the chapter looks forward to promising techniques which might bring some of the success achieved in games such as Go and Chess, to real-time video games. For simplicity, we will consider primarily the objective of maximising playing strength, and consider games where this is a challenging task, which results in interesting gameplay.</td>
</tr>
<tr id="bib_Cowling2013Search" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Cowling2013Search,
  author = {Cowling, Peter I. and Buro, Michael and Bida, Michal and Botea, Adi and Bouzy, Bruno and Butz, Martin V. and Hingston, Philip and Mu&ntilde;oz-Avila, Hector and Nau, Dana and Sipper, Moshe},
  title = {Search in Real-Time Video Games},
  booktitle = {Artificial and Computational Intelligence in Games},
  publisher = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  year = {2013},
  volume = {6},
  pages = {1--19}
}
</pre></td>
</tr>
<tr id="Cowling2012Information" class="entry">
	<td>Cowling, P.I., Powley, E.J. and Whitehouse, D.</td>
	<td>Information Set Monte Carlo Tree Search <p class="infolinks">[<a href="javascript:toggleInfo('Cowling2012Information','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Cowling2012Information','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>IEEE Trans. on Computational Intelligence and AI in Games (CAIG'12)<br/>Vol. 4(2), pp. 120-143&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Cowling2012Information" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Monte Carlo tree search (MCTS) is an AI technique that has been successfully applied to many deterministic games of perfect information. This paper investigates the application of MCTS methods to games with hidden information and uncertainty.<p>In particular, three new information set MCTS (ISMCTS) algorithms are presented which handle different sources of hidden information and uncertainty in games. Instead of searching minimax trees of game states, the ISMCTS algorithms search trees of information sets, more directly analyzing the true structure of the game. These algorithms are tested in three domains with different characteristics, and it is demonstrated that our new algorithms outperform existing approaches to handling hidden information and uncertainty in games.</td>
</tr>
<tr id="bib_Cowling2012Information" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Cowling2012Information,
  author = {Cowling, Peter I and Powley, Edward J and Whitehouse, Daniel},
  title = {Information Set Monte Carlo Tree Search},
  journal = {IEEE Trans. on Computational Intelligence and AI in Games (CAIG'12)},
  publisher = {IEEE},
  year = {2012},
  volume = {4},
  number = {2},
  pages = {120--143}
}
</pre></td>
</tr>
<tr id="Croonenborghs2008Learning" class="entry">
	<td>Croonenborghs, T., Driessens, K. and Bruynooghe, M.</td>
	<td>Learning relational options for inductive transfer in relational reinforcement learning <p class="infolinks">[<a href="javascript:toggleInfo('Croonenborghs2008Learning','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Croonenborghs2008Learning','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>Inductive Logic Programming, pp. 88-97&nbsp;</td>
	<td>incollection</td>
</tr>
<tr id="abs_Croonenborghs2008Learning" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In reinforcement learning problems, an agent has the task of learning a good or optimal strategy from interaction with his environment. At the start of the learning task, the agent usually has very little information. Therefore, when faced with complex problems that have a large state space, learning a good strategy might be infeasible or too slow to work in practice. One way to overcome this problem, is the use of guidance to supply the agent with traces of “reasonable policies”. However, in a lot of cases it will be hard for the user to supply such a policy. In this paper, we will investigate the use of transfer learning in Relational Reinforcement Learning. The goal of transfer learning is to accelerate learning on a target task after training on a different, but related, source task. More specifically, we introduce an extension of the options framework to the relational setting and show how one can learn skills that can be transferred across similar, but different domains. We present experiments showing the possible benefits of using relational options for transfer learning.</td>
</tr>
<tr id="bib_Croonenborghs2008Learning" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Croonenborghs2008Learning,
  author = {Croonenborghs, Tom and Driessens, Kurt and Bruynooghe, Maurice},
  title = {Learning relational options for inductive transfer in relational reinforcement learning},
  booktitle = {Inductive Logic Programming},
  publisher = {Springer},
  year = {2008},
  pages = {88--97}
}
</pre></td>
</tr>
<tr id="Croonenborghs2007Online" class="entry">
	<td>Croonenborghs, T., Ramon, J., Blockeel, H. and Bruynooghe, M.</td>
	<td>Online Learning and Exploiting Relational Models in Reinforcement Learning. <p class="infolinks">[<a href="javascript:toggleInfo('Croonenborghs2007Online','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Croonenborghs2007Online','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td><br/>Vol. 2007Proc. International Joint Conference on Artificial Intelligence, pp. 726-731&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Croonenborghs2007Online" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In recent years, there has been a growing interest in using rich representations such as relational languages for reinforcement learning. However, while expressive languages have many advantages in terms of generalization and reasoning, extending existing approaches to such a relational setting is a non-trivial problem. In this paper, we present a first step towards the online learning and exploitation of relational models. We propose a representation for the transition and reward function that can be learned online and present a method that exploits these models by augmenting Relational Reinforcement Learning algorithms with planning techniques. The benefits and robustness of our approach are evaluated experimentally</td>
</tr>
<tr id="bib_Croonenborghs2007Online" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Croonenborghs2007Online,
  author = {Croonenborghs, Tom and Ramon, Jan and Blockeel, Hendrik and Bruynooghe, Maurice},
  title = {Online Learning and Exploiting Relational Models in Reinforcement Learning.},
  booktitle = {Proc. International Joint Conference on Artificial Intelligence},
  year = {2007},
  volume = {2007},
  pages = {726--731}
}
</pre></td>
</tr>
<tr id="Dzeroski2001Relational" class="entry">
	<td>D&#382;eroski, S., De Raedt, L. and Driessens, K.</td>
	<td>Relational reinforcement learning <p class="infolinks">[<a href="javascript:toggleInfo('Dzeroski2001Relational','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Dzeroski2001Relational','bibtex')">BibTeX</a>]</p></td>
	<td>2001</td>
	<td>Machine learning<br/>Vol. 43(1-2), pp. 7-52&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Dzeroski2001Relational" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Relational reinforcement learning is presented, a learning technique that combines reinforcement learning with relational learning or inductive logic programming. Due to the use of a more expressive representation language to represent states, actions and Q-functions, relational reinforcement learning can be potentially applied to a new range of learning tasks. One such task that we investigate is planning in the blocks world, where it is assumed that the effects of the actions are unknown to the agent and the agent has to learn a policy. Within this simple domain we show that relational reinforcement learning solves some existing problems with reinforcement learning. In particular, relational reinforcement learning allows us to employ structural representations, to abstract from specific goals pursued and to exploit the results of previous learning phases when addressing new (more complex) situations.</td>
</tr>
<tr id="bib_Dzeroski2001Relational" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Dzeroski2001Relational,
  author = {D&#382;eroski, Sa&#353;o and De Raedt, Luc and Driessens, Kurt},
  title = {Relational reinforcement learning},
  journal = {Machine learning},
  publisher = {Springer},
  year = {2001},
  volume = {43},
  number = {1-2},
  pages = {7--52}
}
</pre></td>
</tr>
<tr id="Devlin2011Theoretical" class="entry">
	<td>Devlin, S. and Kudenko, D.</td>
	<td>Theoretical considerations of potential-based reward shaping for multi-agent systems <p class="infolinks">[<a href="javascript:toggleInfo('Devlin2011Theoretical','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Devlin2011Theoretical','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td><br/>Vol. 1Proc. International Conference on Autonomous Agents and Multiagent Systems, pp. 225-232&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Devlin2011Theoretical" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Potential-based reward shaping has previously been proven to both be equivalent to Q-table initialisation and guarantee policy invariance in single-agent reinforcement learning. The method has since been used in multi-agent reinforcement learning without consideration of whether the theoretical equivalence and guarantees hold. This paper extends the existing proofs to similar results in multi-agent systems, providing the theoretical background to explain the success of previous empirical studies. Specically, it is proven that the equivalence to Q-table initialisation remains and the Nash Equilibria of the underlying stochastic game are not modied. Furthermore, we demonstrate empirically that potential-based reward shaping eects exploration and, consequentially, can alter the joint policy converged upon.</td>
</tr>
<tr id="bib_Devlin2011Theoretical" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Devlin2011Theoretical,
  author = {Devlin, Sam and Kudenko, Daniel},
  title = {Theoretical considerations of potential-based reward shaping for multi-agent systems},
  booktitle = {Proc. International Conference on Autonomous Agents and Multiagent Systems},
  year = {2011},
  volume = {1},
  pages = {225--232}
}
</pre></td>
</tr>
<tr id="Dietterich2000Hierarchical" class="entry">
	<td>Dietterich, T.G.</td>
	<td>Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition <p class="infolinks">[<a href="javascript:toggleInfo('Dietterich2000Hierarchical','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Dietterich2000Hierarchical','bibtex')">BibTeX</a>]</p></td>
	<td>2000</td>
	<td>Journal of Artificial Intelligence Research<br/>Vol. 13, pp. 227-303&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Dietterich2000Hierarchical" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics---as a subroutine hierarchy---and a declarative semantics---as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and define subtasks that achieve these subgoals. By defining such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the method. This paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges with probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.</td>
</tr>
<tr id="bib_Dietterich2000Hierarchical" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Dietterich2000Hierarchical,
  author = {Dietterich, Thomas G.},
  title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
  journal = {Journal of Artificial Intelligence Research},
  year = {2000},
  volume = {13},
  pages = {227--303}
}
</pre></td>
</tr>
<tr id="Dietterich1998MAXQ" class="entry">
	<td>Dietterich, T.G.</td>
	<td>The MAXQ Method for Hierarchical Reinforcement Learning. <p class="infolinks">[<a href="javascript:toggleInfo('Dietterich1998MAXQ','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Dietterich1998MAXQ','bibtex')">BibTeX</a>]</p></td>
	<td>1998</td>
	<td>Proc. International Conference on Machine Learning, pp. 118-126&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Dietterich1998MAXQ" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a new approach to hierarchical reinforcement learning based on the MAXQ decomposition of the value function. The MAXQ decomposition has both a procedural semantics—as a subroutine hierarchy—and a declarative semantics—as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. Conditions under which the MAXQ decomposition can represent the optimal value function are derived. The paper defines a hierarchical Q learning algorithm, proves its convergence, and shows experimentally that it can learn much faster than ordinary “flat” Q learning. Finally, the paper discusses some interesting issues that arise in hierarchical reinforcement learning including the hierarchical credit assignment problem and non-hierarchical execution of the MAXQ hierarchy.</td>
</tr>
<tr id="bib_Dietterich1998MAXQ" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Dietterich1998MAXQ,
  author = {Dietterich, Thomas G},
  title = {The MAXQ Method for Hierarchical Reinforcement Learning.},
  booktitle = {Proc. International Conference on Machine Learning},
  year = {1998},
  pages = {118--126}
}
</pre></td>
</tr>
<tr id="Efthymiadis2013Abstract" class="entry">
	<td>Efthymiadis, K., Devlin, S. and Kudenko, D.</td>
	<td>Abstract MDP Reward Shaping for Multi-Agent Reinforcement Learning <p class="infolinks">[<a href="javascript:toggleInfo('Efthymiadis2013Abstract','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Efthymiadis2013Abstract','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Proc. European Workshop of Multi-agent Systems&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Efthymiadis2013Abstract" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reward shaping has been shown to significantly improve an agent’s performance in reinforcement learning. As attention is shifting from tabula-rasa approaches to methods where some heuristic domain knowledge can be given to agents, an important problem that arises is how to generate a useful potential function. Previous research demonstrated the use of plan-based reward shaping in multi-agent reinforcement learning (MARL), where STRIPS planning was used to generate a potential function. The results showed that potential functions based on joint plans can improve an agent’s performance. When using individual plans however, the agents face conflicting goals which can have a detrimental effect in performance. In this paper we present the use of abstract MDPs as a method to provide heuristic knowledge in MARL and how it can be utilised for conflict resolution when agent communication and goal-shaping is not possible.</td>
</tr>
<tr id="bib_Efthymiadis2013Abstract" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Efthymiadis2013Abstract,
  author = {Efthymiadis, Kyriakos and Devlin, Sam and Kudenko, Daniel},
  title = {Abstract MDP Reward Shaping for Multi-Agent Reinforcement Learning},
  booktitle = {Proc. European Workshop of Multi-agent Systems},
  year = {2013}
}
</pre></td>
</tr>
<tr id="Efthymiadis2012Overcoming" class="entry">
	<td>Efthymiadis, K., Devlin, S. and Kudenko, D.</td>
	<td>Overcoming Incorrect Knowledge in Plan-Based Reward Shaping <p class="infolinks">[<a href="javascript:toggleInfo('Efthymiadis2012Overcoming','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Efthymiadis2012Overcoming','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Proc. AAMAS Adapative Learning Agents Worshop, pp. 65-72&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Efthymiadis2012Overcoming" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reward shaping has been shown to significantly improve an agent's performance in reinforcement learning. Plan-based reward shaping is a successful approach in which a STRIPS plan is used in order to guide the agent to the optimal behaviour. However, if the provided knowledge is wrong, it has been shown the agent will take longer to learn the optimal policy. Previously, in some cases, it was better to ignore all prior knowledge despite it only being partially incorrect.<p>This paper introduces a novel use of knowledge revision to overcome incorrect domain knowledge when provided to an agent receiving plan-based reward shaping. Empirical results show that an agent using this method can outperform the previous agent receiving plan-based reward shaping without knowledge revision.</td>
</tr>
<tr id="bib_Efthymiadis2012Overcoming" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Efthymiadis2012Overcoming,
  author = {Efthymiadis, Kyriakos and Devlin, Sam and Kudenko, Daniel},
  title = {Overcoming Incorrect Knowledge in Plan-Based Reward Shaping},
  booktitle = {Proc. AAMAS Adapative Learning Agents Worshop},
  year = {2012},
  pages = {65--72}
}
</pre></td>
</tr>
<tr id="Efthymiadis2014Comparison" class="entry">
	<td>Efthymiadis, K. and Kudenko, D.</td>
	<td>A Comparison of Plan-Based and Abstract MDP Reward Shaping <p class="infolinks">[<a href="javascript:toggleInfo('Efthymiadis2014Comparison','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Efthymiadis2014Comparison','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Connection Science<br/>Vol. 26(1), pp. 85-99&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Efthymiadis2014Comparison" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reward shaping has been shown to significantly improve an agent's performance in reinforcement learning. As attention is shifting away from tabula-rasa approaches many diferent reward shaping methods have been developed.<p>In this paper we compare two different methods for reward shaping; plan-based, in which an agent is provided with a plan and extra rewards are given according to the steps of the plan the agent satisfies, and reward shaping via abstract MDPs, in which an abstract high-level MDP of the environment is solved and the resulting value function is used to shape the agent. The comparison is conducted in terms of total reward, convergence speed and scaling up to more complex environments.<p>Empirical results demonstrate the need to correctly select and set up reward shaping methods according to the needs of the environment the agents are acting in. This leads to the more interesting question, is there a reward shaping method which is universally better than all other approaches regardless of the environment dynamics?</td>
</tr>
<tr id="bib_Efthymiadis2014Comparison" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Efthymiadis2014Comparison,
  author = {Efthymiadis, Kyriakos and Kudenko, Daniel},
  title = {A Comparison of Plan-Based and Abstract MDP Reward Shaping},
  journal = {Connection Science},
  publisher = {Taylor &amp; Francis},
  year = {2014},
  volume = {26},
  number = {1},
  pages = {85--99}
}
</pre></td>
</tr>
<tr id="Efthymiadis2013Using" class="entry">
	<td>Efthymiadis, K. and Kudenko, D.</td>
	<td>Using Plan-Based Reward Shaping to Learn Strategies in StarCraft: Broodwar <p class="infolinks">[<a href="javascript:toggleInfo('Efthymiadis2013Using','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Efthymiadis2013Using','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Proc. IEEE Conference on Computational Intelligence in Games, pp. 1-8&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Efthymiadis2013Using" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: StarCraft: Broodwar (SC:BW) is a very popular commercial real strategy game (RTS) which has been extensively used in AI research. Despite being a popular test-bed reinforcement learning (RL) has not been evaluated extensively. A successful attempt was made to show the use of RL in a small-scale combat scenario involving an overpowered agent battling against multiple enemy units [1]. However, the chosen scenario was very small and not representative of the complexity of the game in its entirety. In order to build an RL agent that can manage the complexity of the full game, more efficient approaches must be used to tackle the state-space explosion. In this paper, we demonstrate how plan-based reward shaping can help an agent scale up to larger, more complex scenarios and significantly speed up the learning process as well as how high level planning can be combined with learning focusing on learning the Starcraft strategy, Battlecruiser Rush. We empirically show that the agent with plan-based reward shaping is significantly better both in terms of the learnt policy, as well as convergence speed when compared to baseline approaches which fail at reaching a good enough policy within a practical amount of time.</td>
</tr>
<tr id="bib_Efthymiadis2013Using" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Efthymiadis2013Using,
  author = {Efthymiadis, Kyriakos and Kudenko, Daniel},
  title = {Using Plan-Based Reward Shaping to Learn Strategies in StarCraft: Broodwar},
  booktitle = {Proc. IEEE Conference on Computational Intelligence in Games},
  year = {2013},
  pages = {1--8}
}
</pre></td>
</tr>
<tr id="Furtak2013Recursive" class="entry">
	<td>Furtak, T. and Buro, M.</td>
	<td>Recursive Monte Carlo Search for Imperfect Information Games <p class="infolinks">[<a href="javascript:toggleInfo('Furtak2013Recursive','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Furtak2013Recursive','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Proc. IEEE Conference on Computational Intelligence in Games, pp. 1-8&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Furtak2013Recursive" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Perfect information Monte Carlo (PIMC) search is the method of choice for constructing strong AI systems for trick-taking card games. PIMC search evaluates moves in imperfect information games by repeatedly sampling worlds based on state inference and estimating move values by solving the corresponding perfect information scenarios. PIMC search performs well in trick-taking card games despite the fact that it suffers from the strategy fusion problem, whereby the game’s information set structure is ignored because moves are evaluated opportunistically in each world. In this paper we describe imperfect information Monte Carlo (IIMC) search, which aims at mitigating this problem by basing move evaluation on more realistic playout sequences rather than perfect information move values. We show that RecPIMC — a recursive IIMC search variant based on perfect information evaluation — performs considerably better than PIMC search in a large class of synthetic imperfect information games and the popular card game of Skat, for which PIMC search is the state-of-the-art cardplay algorithm.</td>
</tr>
<tr id="bib_Furtak2013Recursive" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Furtak2013Recursive,
  author = {Furtak, Timothy and Buro, Michael},
  title = {Recursive Monte Carlo Search for Imperfect Information Games},
  booktitle = {Proc. IEEE Conference on Computational Intelligence in Games},
  year = {2013},
  pages = {1--8}
}
</pre></td>
</tr>
<tr id="Gornicki2012Using" class="entry">
	<td>G&oacute;rnicki, K.</td>
	<td>Using heuristics for Monte Carlo Tree Search <p class="infolinks">[<a href="javascript:toggleInfo('Gornicki2012Using','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Gornicki2012Using','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td><i>School</i>: University of York&nbsp;</td>
	<td>mastersthesis</td>
</tr>
<tr id="abs_Gornicki2012Using" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Monte Carlo Tree Search has been shown to be highly successful on very challenging games, such as, Go. The method is based on randomly sampling sequences of moves to the end of the game and assigning a score to the next move based on the outcomes of these samples. Experiments with Go have shown that biasing the sampling with heuristic expert knowledge did not help but rather hurt performance. <p>In this paper, we exercise the application of heuristic evaluation to Monte Carlo Tree Search algorithm for the ancient game PahTum. At first, we construct brand new heuristics and implement AI agent solely relying on it without performing any roll-outs. In the match of 100 games it outperformed UCT algorithm proposed by Kocsis and Szepesv´ari by winning 99% of the games. Then we demonstrate various techniques of how aforementioned heuristic evaluation function can be incorporated to the Monte Carlo Tree Search algorithm in order to improve the quality of play. The most successful implementation in the match of 100 games won over heuristic-based agent 87% of the games and 100% against UCT with 5 times smaller number of roll-outs in the match of 400 games.</td>
</tr>
<tr id="bib_Gornicki2012Using" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@mastersthesis{Gornicki2012Using,
  author = {G&oacute;rnicki, Karol},
  title = {Using heuristics for Monte Carlo Tree Search},
  school = {University of York},
  year = {2012}
}
</pre></td>
</tr>
<tr id="Gelly2011Monte" class="entry">
	<td>Gelly, S. and Silver, D.</td>
	<td>Monte-Carlo Tree Search and Rapid Action Value Estimation in Computer Go <p class="infolinks">[<a href="javascript:toggleInfo('Gelly2011Monte','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Gelly2011Monte','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Artificial Intelligence<br/>Vol. 175(11), pp. 1856 - 1875&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Gelly2011Monte" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A new paradigm for search, based on Monte-Carlo simulation, has revolutionised the performance of computer Go programs. In this article we describe two extensions to the Monte-Carlo tree search algorithm, which significantly improve the effectiveness of the basic algorithm. When we applied these two extensions to the Go program MoGo, it became the first program to achieve dan (master) level in 9 × 9 Go. In this article we survey the Monte-Carlo revolution in computer Go, outline the key ideas that led to the success of MoGo and subsequent Go programs, and provide for the first time a comprehensive description, in theory and in practice, of this extended framework for Monte-Carlo tree search. </td>
</tr>
<tr id="bib_Gelly2011Monte" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Gelly2011Monte,
  author = {Gelly, Sylvain and Silver, David},
  title = {Monte-Carlo Tree Search and Rapid Action Value Estimation in Computer Go},
  journal = {Artificial Intelligence},
  year = {2011},
  volume = {175},
  number = {11},
  pages = {1856 - 1875}
}
</pre></td>
</tr>
<tr id="Gelly2007Combining" class="entry">
	<td>Gelly, S. and Silver, D.</td>
	<td>Combining Online and Offline Knowledge in UCT <p class="infolinks">[<a href="javascript:toggleInfo('Gelly2007Combining','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Gelly2007Combining','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>Proc. IMLS International Conference on Machine Learning, pp. 273-280&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Gelly2007Combining" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The UCT algorithm learns a value function online using sample-based search. The TD(lambda) algorithm can learn a value function offline for the on-policy distribution. We consider three approaches for combining offline and online value functions in the UCT algorithm. First, the offline value function is used as a default policy during Monte-Carlo simulation. Second, the UCT value function is combined with a rapid online estimate of action values. Third, the offline value function is used as prior knowledge in the UCT search tree. We evaluate these algorithms in 9 x 9 Go against GnuGo 3.7.10. The first algorithm performs better than UCT with a random simulation policy, but surprisingly, worse than UCT with a weaker, handcrafted simulation policy. The second algorithm outperforms UCT altogether. The third algorithm outperforms UCT with handcrafted prior knowledge. We combine these algorithms in MoGo, the world's strongest 9 x 9 Go program. Each technique significantly improves MoGo's playing strength.</td>
</tr>
<tr id="bib_Gelly2007Combining" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Gelly2007Combining,
  author = {Gelly, Sylvain and Silver, David},
  title = {Combining Online and Offline Knowledge in UCT},
  booktitle = {Proc. IMLS International Conference on Machine Learning},
  year = {2007},
  pages = {273--280}
}
</pre></td>
</tr>
<tr id="Gelly2006Modification" class="entry">
	<td>Gelly, S., Wang, Y., Munos, R&eacute;. and Teytaud, O.</td>
	<td>Modification of UCT with patterns in Monte-Carlo Go <p class="infolinks">[<a href="javascript:toggleInfo('Gelly2006Modification','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Gelly2006Modification','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>(RR-6062)&nbsp;</td>
	<td>techreport</td>
</tr>
<tr id="abs_Gelly2006Modification" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Algorithm UCB1 for multi-armed bandit problem has already been extended to Algorithm UCT (Upper bound Confidence for Tree) which works for minimax tree search. We have developed a Monte-Carlo Go program, MoGo, which is the first computer Go program using UCT. We explain our modification of UCT for Go application and also the intelligent random simulation with patterns which has improved significantly the performance of MoGo. UCT combined with pruning techniques for large Go board is discussed, as well as parallelization of UCT. MoGo is now a top level Go program on 9×9 and 13×13 Go boards.</td>
</tr>
<tr id="bib_Gelly2006Modification" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@techreport{Gelly2006Modification,
  author = {Gelly, Sylvain and Wang, Yizao and Munos, R&eacute;mi and Teytaud, Olivier},
  title = {Modification of UCT with patterns in Monte-Carlo Go},
  year = {2006},
  number = {RR-6062},
  note = {inria-00117266v3}
}
</pre></td>
</tr>
<tr id="Grzes2010Online" class="entry">
	<td>Grze&#347;, M. and Kudenko, D.</td>
	<td>Online learning of shaping rewards in reinforcement learning <p class="infolinks">[<a href="javascript:toggleInfo('Grzes2010Online','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Grzes2010Online','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>Neural Networks<br/>Vol. 23(4), pp. 541-550&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Grzes2010Online" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Potential-based reward shaping has been shown to be a powerful method to improve the convergence rate of reinforcement learning agents. It is a flexible technique to incorporate background knowledge into temporal-difference learning in a principled way. However, the question remains of how to compute the potential function which is used to shape the reward that is given to the learning agent. In this paper, we show how, in the absence of knowledge to define the potential function manually, this function can be learned online in parallel with the actual reinforcement learning process. Two cases are considered. The first solution which is based on the multi-grid discretisation is designed for model-free reinforcement learning. In the second case, the approach for the prototypical model-based R-max algorithm is proposed. It learns the potential function using the free space assumption about the transitions in the environment. Two novel algorithms are presented and evaluated.</td>
</tr>
<tr id="bib_Grzes2010Online" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Grzes2010Online,
  author = {Grze&#347;, Marek and Kudenko, Daniel},
  title = {Online learning of shaping rewards in reinforcement learning},
  journal = {Neural Networks},
  publisher = {Elsevier},
  year = {2010},
  volume = {23},
  number = {4},
  pages = {541--550}
}
</pre></td>
</tr>
<tr id="Grzes2008Multigrid" class="entry">
	<td>Grze&#347;, M. and Kudenko, D.</td>
	<td>Multigrid Reinforcement Learning with Reward Shaping <p class="infolinks">[<a href="javascript:toggleInfo('Grzes2008Multigrid','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Grzes2008Multigrid','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>Artificial Neural Networks, pp. 357-366&nbsp;</td>
	<td>incollection</td>
</tr>
<tr id="abs_Grzes2008Multigrid" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Potential-based reward shaping has been shown to be a powerful method to improve the convergence rate of reinforcement learning agents. It is a flexible technique to incorporate background knowledge into temporal-difference learning in a principled way. However, the question remains how to compute the potential which is used to shape the reward that is given to the learning agent. In this paper we propose a way to solve this problem in reinforcement learning with state space discretisation. In particular, we show that the potential function can be learned online in parallel with the actual reinforcement learning process. If the Q-function is learned for states determined by a given grid, a V-function for states with lower resolution can be learned in parallel and used to approximate the potential for ground learning. The novel algorithm is presented and experimentally evaluated.</td>
</tr>
<tr id="bib_Grzes2008Multigrid" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Grzes2008Multigrid,
  author = {Grze&#347;, Marek and Kudenko, Daniel},
  title = {Multigrid Reinforcement Learning with Reward Shaping},
  booktitle = {Artificial Neural Networks},
  publisher = {Springer},
  year = {2008},
  pages = {357--366}
}
</pre></td>
</tr>
<tr id="Grzes2008Plan" class="entry">
	<td>Grze&#347;, M. and Kudenko, D.</td>
	<td>Plan-Based Reward Shaping for Reinforcement Learning <p class="infolinks">[<a href="javascript:toggleInfo('Grzes2008Plan','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Grzes2008Plan','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td><br/>Vol. 2Proc. IEEE Conference on Intelligent Systems, pp. 10-22&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Grzes2008Plan" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement learning, while being a highly popular learning technique for agents and multi-agent systems, has so far encountered difficulties when applying it to more complex domains due to scaling-up problems. This paper focuses on the use of domain knowledge to improve the convergence speed and optimality of various RL techniques. Specifically, we propose the use of high-level STRIPS operator knowledge in reward shaping to focus the search for the optimal policy. Empirical results show that the plan-based reward shaping approach outperforms other RL techniques, including alternative manual and MDP-based reward shaping when it is used in its basic form. We show that MDP-based reward shaping may fail and successful experiments with STRIPS-based shaping suggest modifications which can overcome encountered problems. The STRIPS-based method we propose allows expressing the same domain knowledge in a different way and the domain expert can choose whether to define an MDP or STRIPS planning task. We also evaluate the robustness of the proposed STRIPS-based technique to errors in the plan knowledge.</td>
</tr>
<tr id="bib_Grzes2008Plan" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Grzes2008Plan,
  author = {Grze&#347;, Marek and Kudenko, Daniel},
  title = {Plan-Based Reward Shaping for Reinforcement Learning},
  booktitle = {Proc. IEEE Conference on Intelligent Systems},
  year = {2008},
  volume = {2},
  pages = {10--22}
}
</pre></td>
</tr>
<tr id="Guhe2014Game" class="entry">
	<td>Guhe, M. and Lascarides, A.</td>
	<td>Game strategies for The Settlers of Catan <p class="infolinks">[<a href="javascript:toggleInfo('Guhe2014Game','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Guhe2014Game','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Proc. IEEE Conference on Computational Intelligence and Games, pp. 1-8&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Guhe2014Game" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present an empirical framework for testing game strategies in The Settlers of Catan, a complex win–lose game that lacks any analytic solution. This framework provides the means to change different components of an autonomous agent’s strategy, and to test them in suitably controlled ways via performance metrics in game simulations and via comparisons of the agent’s behaviours with those exhibited in a corpus of humans playing the game. We provide changes to the game strategy that not only improve the agent’s strength, but corpus analysis shows that they also bring the agent closer to a model of human players.</td>
</tr>
<tr id="bib_Guhe2014Game" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Guhe2014Game,
  author = {Guhe, Markus and Lascarides, Alex},
  title = {Game strategies for The Settlers of Catan},
  booktitle = {Proc. IEEE Conference on Computational Intelligence and Games},
  year = {2014},
  pages = {1--8}
}
</pre></td>
</tr>
<tr id="Heinrich2014Self" class="entry">
	<td>Heinrich, J. and Silver, D.</td>
	<td>Self-Play Monte-Carlo Tree Search in Computer Poker <p class="infolinks">[<a href="javascript:toggleInfo('Heinrich2014Self','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Heinrich2014Self','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Proc. AAAI Workshop on Computer Poker and Imperfect Information&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Heinrich2014Self" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Self-play reinforcement learning has proved to be successful in many perfect information two-player games. However, research carrying over its theoretical guarantees and practical success to games of imperfect information has been lacking. In this paper, we evaluate selfplay Monte-Carlo Tree Search (MCTS) in limit Texas Hold’em and Kuhn poker. We introduce a variant of the established UCB algorithm and provide first empirical results demonstrating its ability to find approximate Nash equilibria.</td>
</tr>
<tr id="bib_Heinrich2014Self" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Heinrich2014Self,
  author = {Heinrich, Johannes and Silver, David},
  title = {Self-Play Monte-Carlo Tree Search in Computer Poker},
  booktitle = {Proc. AAAI Workshop on Computer Poker and Imperfect Information},
  year = {2014}
}
</pre></td>
</tr>
<tr id="Hengst2002Discovering" class="entry">
	<td>Hengst, B.</td>
	<td>Discovering Hierarchy in Reinforcement Learning with HEXQ <p class="infolinks">[<a href="javascript:toggleInfo('Hengst2002Discovering','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hengst2002Discovering','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td><br/>Vol. 2Proc. IMLS International Conference on Machine Learning, pp. 243-250&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Hengst2002Discovering" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: An open problem in reinforcement learning is discovering hierarchical structure. HEXQ, an algorithm which automatically attempts to decompose and solve a model-free factored MDP hierarchically is described. By searching for aliased Markov sub-space regions based on the state variables the algorithm uses temporal and state abstraction to construct a hierarchy of interlinked smaller MDPs.</td>
</tr>
<tr id="bib_Hengst2002Discovering" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Hengst2002Discovering,
  author = {Hengst, Bernhard},
  title = {Discovering Hierarchy in Reinforcement Learning with HEXQ},
  booktitle = {Proc. IMLS International Conference on Machine Learning},
  year = {2002},
  volume = {2},
  pages = {243--250}
}
</pre></td>
</tr>
<tr id="Hostetler2014State" class="entry">
	<td>Hostetler, J., Fern, A. and Dietterich, T.</td>
	<td>State Aggregation in Monte Carlo Tree Search. <p class="infolinks">[<a href="javascript:toggleInfo('Hostetler2014State','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hostetler2014State','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Proc. AAAI Conference on Artificial Intelligence, pp. 2446-2452&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Hostetler2014State" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Monte Carlo tree search (MCTS) algorithms are a popular approach to online decision-making in Markov decision processes (MDPs). These algorithms can, however, perform poorly in MDPs with high stochastic branching factors. In this paper, we study state aggregation as a way of reducing stochastic branching in tree search. Prior work has studied formal properties of MDP state aggregation in the context of dynamic programming and reinforcement learning, but little attention has been paid to state aggregation in MCTS. Our main result is a performance loss bound for a class of value function-based state aggregation criteria in expectimax search trees. We also consider how to construct MCTS algorithms that operate in the abstract state space but require a simulator of the ground dynamics only.We find that trajectory sampling algorithms like UCT can be adapted easily, but that sparse sampling algorithms present difficulties. As a proof of concept, we experimentally confirm that state aggregation can improve the finite-sample performance of UCT.</td>
</tr>
<tr id="bib_Hostetler2014State" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Hostetler2014State,
  author = {Hostetler, Jesse and Fern, Alan and Dietterich, Tom},
  title = {State Aggregation in Monte Carlo Tree Search.},
  booktitle = {Proc. AAAI Conference on Artificial Intelligence},
  year = {2014},
  pages = {2446-2452}
}
</pre></td>
</tr>
<tr id="Hwang2012Adaptive" class="entry">
	<td>Hwang, K.-S., Chen, Y.-J. and Jiang, W.-C.</td>
	<td>Adaptive State Aggregation for Reinforcement Learning <p class="infolinks">[<a href="javascript:toggleInfo('Hwang2012Adaptive','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hwang2012Adaptive','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Proc. IEEE Conference on Systems, Man, and Cybernetics, pp. 2452-2456&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Hwang2012Adaptive" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: State partition is an important issue in reinforcement learning, because it has a significant effect on the performance. In this paper, an adaptive state partition method is presented for discretizing the state space adaptively and makes use of decision trees effectively. The proposed method splits the state space according to the temporal difference generated by the reinforcement learning. Consequently, the reinforcement learning uses the state space partitioned by the decision tree to learn the policy simultaneously. For avoiding a trivial partition, sibling nodes are pruned according to the Activity and the Reliability. A Monte-Carlo Tree Search (MCTS) is also proposed to explore the policy. A simulation for approaching goal has been conducted to demonstrate that the proposed method can achieve the design goal.</td>
</tr>
<tr id="bib_Hwang2012Adaptive" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Hwang2012Adaptive,
  author = {Kao-Shing Hwang and Yu-Jen Chen and Wei-Cheng Jiang},
  title = {Adaptive State Aggregation for Reinforcement Learning},
  booktitle = {Proc. IEEE Conference on Systems, Man, and Cybernetics},
  year = {2012},
  pages = {2452-2456}
}
</pre></td>
</tr>
<tr id="Johnson2010Playing" class="entry">
	<td>Johnson, S.</td>
	<td>Playing to Lose: AI and Civilization <p class="infolinks">[<a href="javascript:toggleInfo('Johnson2010Playing','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Johnson2010Playing','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>Google Tech Talks&nbsp;</td>
	<td>misc</td>
</tr>
<tr id="abs_Johnson2010Playing" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Artificial intelligence is crucial to any strategy game, providing a compelling opponent for solo play. While many of the challenges of AI development are technical, significant design challenges exist as well. Can the AI behave like a human? Should it? Should the game design be adjusted to accommodate the limitations of the AI? How do we make the AI fun? Should the AI cheat? If so, how much? Do we even want the AI to win? This session suggests some possible answers to these questions using the "Civilization" series as a case study. Ultimately, developers must choose between a "good" AI and a "fun" one, with an understanding of the trade-offs inherent when deciding between the two. <p>Soren Johnson was the lead designer and AI programmer for Sid Meier's Civilization IV. After working at Firaxis Games for seven years, Soren joined EA Maxis in 2007 to work on Spore as a lead designer/programmer. He is currently building web-based games with EA2D, such as the moddable strategystation.com and other unannounced projects. He also writes a design column for Game Developer Magazine and is on the GDC Advisory Board. His thoughts on game design can be found at http://www.designer-notes.com.</td>
</tr>
<tr id="bib_Johnson2010Playing" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@misc{Johnson2010Playing,
  author = {Johnson, Soren},
  title = {Playing to Lose: AI and Civilization},
  year = {2010},
  url = {http://www.youtube.com/watch?v=IJcuQQ1eWWI}
}
</pre></td>
</tr>
<tr id="Kocsis2006Bandit" class="entry">
	<td>Kocsis, L. and Szepesv&aacute;ri, C.</td>
	<td>Bandit based Monte-Carlo planning <p class="infolinks">[<a href="javascript:toggleInfo('Kocsis2006Bandit','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Kocsis2006Bandit','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>Machine Learning: ECML 2006, pp. 282-293&nbsp;</td>
	<td>incollection</td>
</tr>
<tr id="abs_Kocsis2006Bandit" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.</td>
</tr>
<tr id="bib_Kocsis2006Bandit" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Kocsis2006Bandit,
  author = {Kocsis, Levente and Szepesv&aacute;ri, Csaba},
  title = {Bandit based Monte-Carlo planning},
  booktitle = {Machine Learning: ECML 2006},
  publisher = {Springer},
  year = {2006},
  pages = {282--293}
}
</pre></td>
</tr>
<tr id="Koga2013Speeding" class="entry">
	<td>Koga, M.L., Silva, V.F.d., Cozman, F.G. and Costa, A.H.R.</td>
	<td>Speeding-up reinforcement learning through abstraction and transfer learning <p class="infolinks">[<a href="javascript:toggleInfo('Koga2013Speeding','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Koga2013Speeding','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Proc. International Conference on Autonomous Agents and Multiagent Systems, pp. 119-126&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Koga2013Speeding" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We are interested in the following general question: is it possible to abstract knowledge that is generated while learning the solution of a problem, so that this abstraction can accelerate the learning process? Moreover, is it possible to transfer and reuse the acquired abstract knowledge to accelerate the learning process for future similar tasks? We propose a framework for conducting simultaneously two levels of reinforcement learning, where an abstract policy is learned while learning of a concrete policy for the problem, such that both policies are refined through exploration and interaction of the agent with the environment. We explore abstraction both to accelerate the learning process for an optimal concrete policy for the current problem, and to allow the application of the generated abstract policy in learning solutions for new problems. We report experiments in a robot navigation environment that show our framework to be effective in speeding up policy construction for practical problems and in generating abstractions that can be used to accelerate learning in new similar problems.</td>
</tr>
<tr id="bib_Koga2013Speeding" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Koga2013Speeding,
  author = {Koga, Marcelo Li and Silva, Valdinei Freire da and Cozman, Fabio Gagliardi and Costa, Anna Helena Reali},
  title = {Speeding-up reinforcement learning through abstraction and transfer learning},
  booktitle = {Proc. International Conference on Autonomous Agents and Multiagent Systems},
  year = {2013},
  pages = {119--126}
}
</pre></td>
</tr>
<tr id="Lanctot2014Search" class="entry">
	<td>Lanctot, M., Lis, V. and Bowling, M.</td>
	<td>Search in Imperfect Information Games using Online Monte Carlo Counterfactual Regret Minimization <p class="infolinks">[<a href="javascript:toggleInfo('Lanctot2014Search','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Lanctot2014Search','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Proc. AAAI Workshop on Computer Poker and Imperfect Information&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Lanctot2014Search" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Online search in games has always been a core interest of artificial intelligence. Advances made in search for perfect information games (such as Chess, Checkers, Go, and Backgammon) have led to AI capable of defeating the world’s top human experts. Search in imperfect information games (such as Poker, Bridge, and Skat) is significantly more challenging due to the complexities introduced by hidden information. In this paper, we present Online Outcome Sampling (OOS), the first imperfect information search algorithm that is guaranteed to converge to an equilibrium strategy in two-player zero-sum games. We show that OOS avoids common problems encountered by existing search algorithms and we experimentally evaluate its convergence rate and practical performance against benchmark strategies in Liar’s Dice and a variant of Goofspiel. We show that unlike with Information Set Monte Carlo Tree Search (ISMCTS) the exploitability of the strategies produced by OOS decreases as the amount of search time increases. In practice, OOS performs as well as ISMCTS in head-to-head play while producing strategies with lower exploitability given the same search time.</td>
</tr>
<tr id="bib_Lanctot2014Search" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Lanctot2014Search,
  author = {Lanctot, Marc and Lis, Viliam and Bowling, Michael},
  title = {Search in Imperfect Information Games using Online Monte Carlo Counterfactual Regret Minimization},
  booktitle = {Proc. AAAI Workshop on Computer Poker and Imperfect Information},
  year = {2014}
}
</pre></td>
</tr>
<tr id="Liu2015Adapting" class="entry">
	<td>Liu, Y.-C. and Tsuruoka, Y.</td>
	<td>Adapting Improved Upper Confidence Bounds for Monte-Carlo Tree Search <p class="infolinks">[<a href="javascript:toggleInfo('Liu2015Adapting','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Liu2015Adapting','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proc. Internation Conference on Advances in Computer GamesProc. International Conference on Advances in Computer Games&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Liu2015Adapting" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The UCT algorithm, which combines the UCB algorithm and Monte-Carlo Tree Search (MCTS), is currently the most widely used variant of MCTS. Recently, a number of investigations into applying other bandit algorithms to MCTS have produced interesting results. In this research, we will investigate the possibility of combining the improved UCB algorithm, proposed by Auer et al. (2010), with MCTS. However, various characteristics and properties of the improved UCB algorithm may not be ideal for a direct application to MCTS. Therefore, some modifications were made to the improved UCB algorithm, making it more suitable for the task of game tree search. The Mi-UCT algorithm is the application of the modified UCB algorithm applied to trees. The performance of Mi-UCT is demonstrated on the games of 9×9 Go and 9×9 NoGo, and has shown to outperform the plain UCT algorithm when only a small number of playouts are given, and rougly on the same level when more playouts are available.</td>
</tr>
<tr id="bib_Liu2015Adapting" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Liu2015Adapting,
  author = {Liu, Yun-Ching and Tsuruoka, Yoshimasa},
  title = {Adapting Improved Upper Confidence Bounds for Monte-Carlo Tree Search},
  booktitle = {Proc. International Conference on Advances in Computer Games},
  journal = {Proc. Internation Conference on Advances in Computer Games},
  year = {2015}
}
</pre></td>
</tr>
<tr id="Lucas2014Fast" class="entry">
	<td>Lucas, S.M., Samothrakis, S. and Perez, D.</td>
	<td>Fast Evolutionary Adaptation for Monte Carlo Tree Search <p class="infolinks">[<a href="javascript:toggleInfo('Lucas2014Fast','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Lucas2014Fast','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>EvoGames&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Lucas2014Fast" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper describes a new adaptive Monte Carlo Tree Search (MCTS) algorithm that uses evolution to rapidly optimise its performance. An evolutionary algorithm is used as a source of control parameters to modify the behaviour of each iteration (i.e. each simulation or roll-out) of the MCTS algorithm; in this paper we largely restrict this to modifying the behaviour of the random default policy, though it can also be applied to modify the tree policy. This method of tightly integrating evolution into the MCTS algorithm means that evolutionary adaptation occurs on a much faster time-scale than has previously been achieved, and addresses a particular problem with MCTS which frequently occurs in real-time video and control problems: that uniform random roll-outs may be uninformative. Results are presented on the classic mountain car reinforcement learning benchmark and also on a cut-down version of Space Invaders. The results clearly demonstrate the value of the approach, significantly outperforming ``standard'' MCTS in each case. Furthermore, the adaptation is almost immediate, with no perceptual delay as the system learns: the agent frequently performs well from its very first game.</td>
</tr>
<tr id="bib_Lucas2014Fast" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Lucas2014Fast,
  author = {Lucas, Simon M and Samothrakis, Spyridon and Perez, Diego},
  title = {Fast Evolutionary Adaptation for Monte Carlo Tree Search},
  booktitle = {EvoGames},
  year = {2014},
  note = {To appear}
}
</pre></td>
</tr>
<tr id="Mehat2010Combining" class="entry">
	<td>M&eacute;hat, J. and Cazenave, T.</td>
	<td>Combining UCT and nested Monte Carlo search for single-player general game playing <p class="infolinks">[<a href="javascript:toggleInfo('Mehat2010Combining','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mehat2010Combining','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>IEEE Trans. on Computational Intelligence and AI in Games (CAIG'11)<br/>Vol. 2(4), pp. 271-277&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Mehat2010Combining" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Algorithm UCB1 for multi-armed bandit problem has already been extended to<br>Monte-Carlo tree search has recently been very successful for game playing particularly for games where the evaluation of a state is difficult to compute, such as Go or General Games. We compare Nested Monte-Carlo Search (NMC), Upper Confidence bounds for Trees (UCT-T), UCT with transposition tables (UCT+T) and a simple combination of NMC and UCT+T (MAX) on single-player games of the past GGP competitions. We show that transposition tables improve UCT and that MAX is the best of these four algorithms. Using UCT+T, the program Ary won the 2009 GGP competition. MAX and NMC are slight improvements over this 2009 version.</td>
</tr>
<tr id="bib_Mehat2010Combining" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Mehat2010Combining,
  author = {M&eacute;hat, Jean and Cazenave, Tristan},
  title = {Combining UCT and nested Monte Carlo search for single-player general game playing},
  journal = {IEEE Trans. on Computational Intelligence and AI in Games (CAIG'11)},
  publisher = {IEEE},
  year = {2010},
  volume = {2},
  number = {4},
  pages = {271--277}
}
</pre></td>
</tr>
<tr id="Makar2001Hierarchical" class="entry">
	<td>Makar, R., Mahadevan, S. and Ghavamzadeh, M.</td>
	<td>Hierarchical Multi-Agent Reinforcement Learning <p class="infolinks">[<a href="javascript:toggleInfo('Makar2001Hierarchical','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Makar2001Hierarchical','bibtex')">BibTeX</a>]</p></td>
	<td>2001</td>
	<td>Proc. International Conference on Autonomous Agents&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Makar2001Hierarchical" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we investigate the use of hierarchical reinforcement learning to speed up the acquisition of cooperative multi-agent tasks. We extend the MAXQ framework to the multi-agent case. Each agent uses the same MAXQ hierarchy to decompose a task into sub-tasks. Learning is decentralized, with each agent learning three interrelated skills: how to perform subtasks, which order to do them in, and how to coordinate with other agents. Coordination skills among agents are learned by using joint actions at the highest level(s) of the hierarchy. The Q nodes at the highest level(s) of the hierarchy are configured to represent the joint task-action space among multiple agents. In this approach, each agent only knows what other agents are doing at the level of sub-tasks, and is unaware of lower level (primitive) actions. This hierarchical approach allows agents to learn coordination faster by sharing information at the level of sub-tasks, rather than attempting to learn coordination taking into account primitive joint state-action values. We apply this hierarchical multi-agent reinforcement learning algorithm to a complex AGV scheduling task and compare its performance and speed with other learning approaches, including flat multi-agent, single agent using MAXQ, selfish multiple agents using MAXQ (where each agent acts independently without communicating with the other agents), as well as several well-known AGV heuristics like "first come first serve", "highest queue first" and "nearest station first". We also compare the tradeoffs in learning speed vs. performance of modeling joint action values at multiple levels in the MAXQ hierarchy.</td>
</tr>
<tr id="bib_Makar2001Hierarchical" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Makar2001Hierarchical,
  author = {Makar, Rajbala and Mahadevan, Sridhar and Ghavamzadeh, Mohammad},
  title = {Hierarchical Multi-Agent Reinforcement Learning},
  booktitle = {Proc. International Conference on Autonomous Agents},
  year = {2001}
}
</pre></td>
</tr>
<tr id="Mannor2004Dynamic" class="entry">
	<td>Mannor, S., Menache, I., Hoze, A. and Klein, U.</td>
	<td>Dynamic Abstraction in Reinforcement Learning via Clustering <p class="infolinks">[<a href="javascript:toggleInfo('Mannor2004Dynamic','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mannor2004Dynamic','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td>Proc. IMLS International Conference on Machine Learning, pp. 71&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Mannor2004Dynamic" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of “interesting” regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.</td>
</tr>
<tr id="bib_Mannor2004Dynamic" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Mannor2004Dynamic,
  author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
  title = {Dynamic Abstraction in Reinforcement Learning via Clustering},
  booktitle = {Proc. IMLS International Conference on Machine Learning},
  year = {2004},
  pages = {71}
}
</pre></td>
</tr>
<tr id="Marthi2007Automatic" class="entry">
	<td>Marthi, B.</td>
	<td>Automatic shaping and decomposition of reward functions <p class="infolinks">[<a href="javascript:toggleInfo('Marthi2007Automatic','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Marthi2007Automatic','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>Proc. IMLS International Conference on Machine Learning, pp. 601-608&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Marthi2007Automatic" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper investigates the problem of automatically learning how to restructure the reward function of a Markov decision process so as to speed up reinforcement learning. We begin by describing a method that learns a shaped reward function given a set of state and temporal abstractions. Next, we consider decomposition of the per-timestep reward in multieffector problems, in which the overall agent can be decomposed into multiple units that are concurrently carrying out various tasks. We show by example that to find a good reward decomposition, it is often necessary to first shape the rewards appropriately. We then give a function approximation algorithm for solving both problems together. Standard reinforcement learning algorithms can be augmented with our methods, and we show experimentally that in each case, significantly faster learning results.</td>
</tr>
<tr id="bib_Marthi2007Automatic" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Marthi2007Automatic,
  author = {Marthi, Bhaskara},
  title = {Automatic shaping and decomposition of reward functions},
  booktitle = {Proc. IMLS International Conference on Machine Learning},
  year = {2007},
  pages = {601--608}
}
</pre></td>
</tr>
<tr id="McCallum1996Reinforcement" class="entry">
	<td>McCallum, A.K.</td>
	<td>Reinforcement Learning with Selective Perception and Hidden State <p class="infolinks">[<a href="javascript:toggleInfo('McCallum1996Reinforcement','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('McCallum1996Reinforcement','bibtex')">BibTeX</a>]</p></td>
	<td>1996</td>
	<td><i>School</i>: University of Rochester&nbsp;</td>
	<td>phdthesis</td>
</tr>
<tr id="abs_McCallum1996Reinforcement" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement learning is a machine learning framework in which an agent manipulates its environment through a series of actions, and in response to each action, receives a reward value. The agent stores its knowledge about how to choose reward-maximizing actions in a mapping from agent-internal states to actions.<p>Agents often struggle with two opposite, yet intertwined, problems regarding their internal state space. First, the agent’s state space may have “too many distinctions” — meaning that an abundance of perceptual data has resulted in a state space so large that it overwhelms the agent’s limited resources for computation, storage and learning experience. This problem can often be solved if the agent uses selective perception to prune away unnecessary distinctions, and focus its attention only certain features. Second, even though there are “too many distinctions,” the agent’s state space may simultaneously contain “too few distinctions”—meaning that perceptual limitations, (such as ﬁeld of view, acuity and occlusions), have temporarily hidden crucial features of the environment from the agent. This problem, called hidden state, can often be solved by using memory of features from previous views to augment the agent’s perceptual inputs.<p>This dissertation presents algorithms that use selective perception and short-term memory to simultaneously prune and augment the state space provided by the agent’s perceptual inputs. During learning, the agent selects task-relevant state distinctions with a utile distinction test that uses robust statistics to determine when a distinction helps the agent predict reward. The dissertation also advocates using instance-based (or “memory-based”) learning for making efficient use of accumulated experience, and using a tree structure to hold variable-length memories. Four new algorithms are shown to perform a variety of tasks well—in some cases with more than an order-of-magnitude better performance than previous algorithms.</td>
</tr>
<tr id="bib_McCallum1996Reinforcement" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@phdthesis{McCallum1996Reinforcement,
  author = {McCallum, Andrew Kachites},
  title = {Reinforcement Learning with Selective Perception and Hidden State},
  school = {University of Rochester},
  year = {1996}
}
</pre></td>
</tr>
<tr id="Munos2013From" class="entry">
	<td>Munos, R&eacute;.</td>
	<td>From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization and planning <p class="infolinks">[<a href="javascript:toggleInfo('Munos2013From','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Munos2013From','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>&nbsp;</td>
	<td>book</td>
</tr>
<tr id="abs_Munos2013From" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This work covers several aspects of the optimism in the face of uncertainty principle applied to large scale optimization problems under finite numerical budget. The initial motivation for the research reported here originated from the empirical success of the so-called Monte-Carlo Tree Search method popularized in computer-go and further extended to many other games as well as optimization and planning problems. Our objective is to contribute to the development of theoretical foundations of the field by characterizing the complexity of the underlying optimization problems and designing efficient algorithms with performance guarantees. The main idea presented here is that it is possible to decompose a complex decision making problem (such as an optimization problem in a large search space) into a sequence of elementary decisions, where each decision of the sequence is solved using a (stochastic) multi-armed bandit (simple mathematical model for decision making in stochastic environments). This so-called hierarchical bandit approach (where the reward observed by a bandit in the hierarchy is itself the return of another bandit at a deeper level) possesses the nice feature of starting the exploration by a quasi-uniform sampling of the space and then focusing progressively on the most promising area, at different scales, according to the evaluations observed so far, and eventually performing a local search around the global optima of the function. The performance of the method is assessed in terms of the optimality of the returned solution as a function of the number of function evaluations. Our main contribution to the field of function optimization is a class of hierarchical optimistic algorithms designed for general search spaces (such as metric spaces, trees, graphs, Euclidean spaces, ...) with different algorithmic instantiations depending on whether the evaluations are noisy or noiseless and whether some measure of the ''smoothness'' of the function is known or unknown. The performance of the algorithms depend on the local behavior of the function around its global optima expressed in terms of the quantity of near-optimal states measured with some metric. If this local smoothness of the function is known then one can design very efficient optimization algorithms (with convergence rate independent of the space dimension), and when it is not known, we can build adaptive techniques that can, in some cases, perform almost as well as when it is known.</td>
</tr>
<tr id="bib_Munos2013From" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@book{Munos2013From,
  author = {Munos, R&eacute;mi},
  title = {From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization and planning},
  publisher = {Now Publishers Inc},
  year = {2013}
}
</pre></td>
</tr>
<tr id="NeginEntezari2011Subgoal" class="entry">
	<td>Negin Entezari, N.E., Mohammad Ebrahim Shiri, M.E.S. and Parham Moradi, P.M.</td>
	<td>Subgoal discovery in reinforcement learning using local graph clustering <p class="infolinks">[<a href="javascript:toggleInfo('NeginEntezari2011Subgoal','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('NeginEntezari2011Subgoal','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>International Journal of Future Generation Communication and Networking<br/>Vol. 4(3), pp. 13-24&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_NeginEntezari2011Subgoal" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement Learning is an area of machine learning that studies the problem of solving sequential decision making problems. The agent must learn behavior through trial-and-error interaction with a dynamic environment. Learning efficiently in large scale problems and complex tasks demands a decomposition of the original complex task into simple and smaller subtasks. In this paper, we present a subgoal-based method for automatically creating useful skills in reinforcement Learning. Our method identifies subgoals using a local graph clustering algorithm. The main advantage of the proposed algorithm is that only the local information of the graph is considered to cluster the agent state space. Clustering of the transition graphs corresponding to MDPs can be performed in linear time using the proposed method. Subgoals discovered by the algorithm are then used to generate skills using the option framework. Experimental results show that the proposed subgoal discovery algorithm has a dramatic effect on the learning performance.</td>
</tr>
<tr id="bib_NeginEntezari2011Subgoal" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{NeginEntezari2011Subgoal,
  author = {Negin Entezari, Negin Entezari and Mohammad Ebrahim Shiri, Mohammad Ebrahim Shiri and Parham Moradi, Parham Moradi},
  title = {Subgoal discovery in reinforcement learning using local graph clustering},
  journal = {International Journal of Future Generation Communication and Networking},
  publisher = {보안공학연구지원센터 (IJFGCN)},
  year = {2011},
  volume = {4},
  number = {3},
  pages = {13--24}
}
</pre></td>
</tr>
<tr id="Ng1999Policy" class="entry">
	<td>Ng, A.Y., Harada, D. and Russell, S.</td>
	<td>Policy invariance under reward transformations: Theory and application to reward shaping <p class="infolinks">[<a href="javascript:toggleInfo('Ng1999Policy','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ng1999Policy','bibtex')">BibTeX</a>]</p></td>
	<td>1999</td>
	<td><br/>Vol. 99Proc. IMLS International Conference on Machine Learning, pp. 278-287&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Ng1999Policy" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper investigates conditions under which modifications to the reward function of a Markov decision process preserve the optimal policy. It is shown that, besides the positive linear transformation familiar from utility theory, one can add a reward for transitions between states that is expressible as the difference in value of an arbitrary potential function applied to those states. Furthermore, this is shown to be a necessary condition for invariance, in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP. These results shed light on the practice of reward shaping, a method used in reinforcement learning whereby additional training rewards are used to guide the learning agent. In particular, some well-known "bugs" in reward shaping procedures are shown to arise from non-potential-based rewards, and methods are given for constructing shaping potentials corresponding to distance-based and subgoal-based heuristics. We show that such potentials can lead to substantial reductions in learning time.</td>
</tr>
<tr id="bib_Ng1999Policy" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Ng1999Policy,
  author = {Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  title = {Policy invariance under reward transformations: Theory and application to reward shaping},
  booktitle = {Proc. IMLS International Conference on Machine Learning},
  year = {1999},
  volume = {99},
  pages = {278--287}
}
</pre></td>
</tr>
<tr id="Ontanon2013Combinatorial" class="entry">
	<td>Ontanon, S.</td>
	<td>The Combinatorial Multi-Armed Bandit Problem and its Application to Real-Time Strategy Games <p class="infolinks">[<a href="javascript:toggleInfo('Ontanon2013Combinatorial','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ontanon2013Combinatorial','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Proc. AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, pp. 68-75&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Ontanon2013Combinatorial" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Game tree search in games with large branching factors is a notoriously hard problem. In this paper, we address this problem with a new sampling strategy for Monte Carlo Tree Search (MCTS) algorithms, called "Naive Sampling", based on a variant of the Multi-armed Bandit problem called the "Combinatorial Multi-armed Bandit" (CMAB) problem. We present a new MCTS algorithm based on Naive Sampling called NaiveMCTS, and evaluate it in the context of real-time strategy (RTS) games. Our results show that as the branching factor grows, NaiveMCTS performs significantly better than other algorithms.</td>
</tr>
<tr id="bib_Ontanon2013Combinatorial" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Ontanon2013Combinatorial,
  author = {Ontanon, Santiago},
  title = {The Combinatorial Multi-Armed Bandit Problem and its Application to Real-Time Strategy Games},
  booktitle = {Proc. AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  year = {2013},
  pages = {68--75}
}
</pre></td>
</tr>
<tr id="Parr1998Reinforcement" class="entry">
	<td>Parr, R. and Russell, S.</td>
	<td>Reinforcement Learning with Hierarchies of Machines <p class="infolinks">[<a href="javascript:toggleInfo('Parr1998Reinforcement','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Parr1998Reinforcement','bibtex')">BibTeX</a>]</p></td>
	<td>1998</td>
	<td>Proc. Conference on Advances in Neural Information Processing Systems, pp. 1043-1049&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Parr1998Reinforcement" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and “behavior-based ” or “teleo-reactive ” approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.<p>Keywords: HAMQ</td>
</tr>
<tr id="bib_Parr1998Reinforcement" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Parr1998Reinforcement,
  author = {Ronald Parr and Stuart Russell},
  title = {Reinforcement Learning with Hierarchies of Machines},
  booktitle = {Proc. Conference on Advances in Neural Information Processing Systems},
  publisher = {MIT Press},
  year = {1998},
  pages = {1043--1049}
}
</pre></td>
</tr>
<tr id="Perez2014Knowledge" class="entry">
	<td>Perez, D., Samothrakis, S. and Lucas, S.</td>
	<td>Knowledge-based Fast Evolutionary MCTS for General Video Game Playing <p class="infolinks">[<a href="javascript:toggleInfo('Perez2014Knowledge','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Perez2014Knowledge','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Proc. IEEE Conference on Computational Intelligence and Games&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Perez2014Knowledge" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: General Video Game Playing is a game AI domain in which the usage of game-dependent domain knowledge is very limited or even non existent. This imposes obvious difficulties when seeking to create agents able to play sets of different games. Taken more broadly, this issue can be used as an introduction to the field of General Artificial Intelligence. This paper explores the performance of a vanilla Monte Carlo Tree Search algorithm, and analyzes the main difficulties encountered when tackling this kind of scenarios. Modifications are proposed to overcome these issues, strengthening the algorithm's ability to gather and discover knowledge, and taking advantage of past experiences. Results show that the performance of the algorithm is significantly improved, although there remain unresolved problems that require further research. The framework employed in this research is publicly available and will be used in the General Video Game Playing competition at the IEEE Conference on Computational Intelligence and Games in 2014.</td>
</tr>
<tr id="bib_Perez2014Knowledge" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Perez2014Knowledge,
  author = {Perez, Diego and Samothrakis, Spyridon and Lucas, Simon},
  title = {Knowledge-based Fast Evolutionary MCTS for General Video Game Playing},
  booktitle = {Proc. IEEE Conference on Computational Intelligence and Games},
  year = {2014}
}
</pre></td>
</tr>
<tr id="Ramanujan2011Behavior" class="entry">
	<td>Ramanujan, R., Sabharwal, A. and Selman, B.</td>
	<td>On the Behavior of UCT in Synthetic Search Spaces <p class="infolinks">[<a href="javascript:toggleInfo('Ramanujan2011Behavior','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ramanujan2011Behavior','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Proc. International Conference on Automated Planning and Scheduling&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Ramanujan2011Behavior" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: UCT and Minimax are two of the most prominent tree-search based adversarial reasoning strategies for a variety of challenging domains, such as Chess and Go. Their complementary strengths in different domains have been the motivation for several works attempting to achieve a better understanding of their vastly different behavior. Rather than using complex games as a testbed for deriving indirect insights into UCT and Minimax, we propose a relatively simple model on which sampling-based techniques, in particular UCT, significantly outperform Minimax. The simplicity of the model enables novel analytical computations of Minimax’s decision accuracy, while an extension of this model incorporating search traps and correlated heuristic noise confirms that UCT’s performance begins to deteriorate as more and more traps are added.</td>
</tr>
<tr id="bib_Ramanujan2011Behavior" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Ramanujan2011Behavior,
  author = {Ramanujan, Raghuram and Sabharwal, Ashish and Selman, Bart},
  title = {On the Behavior of UCT in Synthetic Search Spaces},
  booktitle = {Proc. International Conference on Automated Planning and Scheduling},
  year = {2011}
}
</pre></td>
</tr>
<tr id="Ramanujan2010Adversarial" class="entry">
	<td>Ramanujan, R., Sabharwal, A. and Selman, B.</td>
	<td>On Adversarial Search Spaces and Sampling-Based Planning. <p class="infolinks">[<a href="javascript:toggleInfo('Ramanujan2010Adversarial','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ramanujan2010Adversarial','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td><br/>Vol. 10Proc. International Conference on Automated Planning and Scheduling, pp. 242-245&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Ramanujan2010Adversarial" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Upper Confidence bounds applied to Trees (UCT), a banditbased Monte-Carlo sampling algorithm for planning, has recently been the subject of great interest in adversarial reasoning. UCT has been shown to outperform traditional minimax based approaches in several challenging domains such as Go and Kriegspiel, although minimax search still prevails in other domains such as Chess. This work provides insights into the properties of adversarial search spaces that play a key role in the success or failure of UCT and similar samplingbased approaches. We show that certain “early loss” or “shallow trap” configurations, while unlikely in Go, occur surprisingly often in games like Chess (even in grandmaster games). We provide evidence that UCT, unlike minimax search, is unable to identify such traps in Chess and spends a great deal of time exploring much deeper game play than needed.</td>
</tr>
<tr id="bib_Ramanujan2010Adversarial" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Ramanujan2010Adversarial,
  author = {Ramanujan, Raghuram and Sabharwal, Ashish and Selman, Bart},
  title = {On Adversarial Search Spaces and Sampling-Based Planning.},
  booktitle = {Proc. International Conference on Automated Planning and Scheduling},
  year = {2010},
  volume = {10},
  pages = {242--245}
}
</pre></td>
</tr>
<tr id="Ravindran2004algebraic" class="entry">
	<td>Ravindran, B.</td>
	<td>An algebraic approach to abstraction in reinforcement learning <p class="infolinks">[<a href="javascript:toggleInfo('Ravindran2004algebraic','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ravindran2004algebraic','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td><i>School</i>: University of Massachusetts Amherst&nbsp;</td>
	<td>phdthesis</td>
</tr>
<tr id="abs_Ravindran2004algebraic" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: To operate effectively in complex environments learning agents require the ability to form useful abstractions, that is, the ability to selectively ignore irrelevant details. Stated in general terms this is a very diffcult problem. Much of the work in this field is specialized to specific modeling paradigms or classes of problems. In this thesis we introduce an abstraction framework for Markov decision processes (MDPs) based on homomorphisms relating MDPs. We build on classical finite-state automata literature and develop a minimization framework for MDPs that can exploit structure and symmetries to derive smaller equivalent models of the problem. Since employing homomorphisms for minimization requires that the resulting abstractions be exact, we introduce approximate and partial homomorphisms and develop bounds for the loss that results from employing relaxed abstraction criteria.<p>Our MDP minimization results can be readily employed by reinforcement learning (RL) methods for forming abstractions. We extend our abstraction approach to hierarchical RL, specifically using the options framework. We introduce relativized options, a generalization of Markov sub-goal options, that allow us to define options without an absolute frame of reference. We introduce an extension to the options framework, based on relativized options, that allows us to learn simultaneously at multiple levels of the hierarchy and also employ hierarchy-specific abstractions. We provide certain theoretical guarantees regarding the performance of hierarchical systems that employ approximate abstraction. We empirically demonstrate the utility of relativized options in several test-beds.<p>Relativized options can also be interpreted as behavioral schemas. We demonstrate that such schemas can be profitably employed in a hierarchical RL setting. We also develop algorithms that learn the appropriate parameter binding to a given schema. We empirically demonstrate the validity and utility of these algorithms. Relativized options allow us to model certain aspects of deictic or indexical representations. We develop a modification of our parameter binding algorithm suited to hierarchical RL architectures that employ deictic representations.</td>
</tr>
<tr id="bib_Ravindran2004algebraic" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@phdthesis{Ravindran2004algebraic,
  author = {Ravindran, Balaraman},
  title = {An algebraic approach to abstraction in reinforcement learning},
  school = {University of Massachusetts Amherst},
  year = {2004}
}
</pre></td>
</tr>
<tr id="Rimmel2010Multiple" class="entry">
	<td>Rimmel, A. and Teytaud, F.</td>
	<td>Multiple overlapping tiles for contextual monte carlo tree search <p class="infolinks">[<a href="javascript:toggleInfo('Rimmel2010Multiple','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Rimmel2010Multiple','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>Applications of Evolutionary Computation, pp. 201-210&nbsp;</td>
	<td>incollection</td>
</tr>
<tr id="abs_Rimmel2010Multiple" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Monte Carlo Tree Search is a recent algorithm that achieves more and more successes in various domains. We propose an improvement of the Monte Carlo part of the algorithm by modifying the simulations depending on the context. The modification is based on a reward function learned on a tiling of the space of Monte Carlo simulations. The tiling is done by regrouping the Monte Carlo simulations where two moves have been selected by one player. We show that it is very efficient by experimenting on the game of Havannah.</td>
</tr>
<tr id="bib_Rimmel2010Multiple" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Rimmel2010Multiple,
  author = {Rimmel, Arpad and Teytaud, Fabien},
  title = {Multiple overlapping tiles for contextual monte carlo tree search},
  booktitle = {Applications of Evolutionary Computation},
  publisher = {Springer},
  year = {2010},
  pages = {201--210}
}
</pre></td>
</tr>
<tr id="Robles2011Learning" class="entry">
	<td>Robles, D., Rohlfshagen, P. and Lucas, S.M.</td>
	<td>Learning Non-Random Moves for Playing Othello: Improving Monte Carlo Tree Search <p class="infolinks">[<a href="javascript:toggleInfo('Robles2011Learning','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Robles2011Learning','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Proc. IEEE Conference on Computational Intelligence in Games, pp. 305-312&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Robles2011Learning" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Monte Carlo Tree Search (MCTS) with an appropriate tree policy may be used to approximate a minimax tree for games such as Go, where a state value function cannot be formulated easily: recent MCTS algorithms successfully combine Upper Confidence Bounds for Trees with Monte Carlo (MC) simulations to incrementally refine estimates on the game-theoretic values of the game's states. Although a game-specific value function is not required for this approach, significant improvements in performance may be achieved by derandomising the MC simulations using domain-specific knowledge. However, recent results suggest that the choice of a non-uniformly random default policy is non-trivial and may often lead to unexpected outcomes.<p>In this paper we employ Temporal Difference Learning (TDL) as a general approach to the integration of domain-specific knowledge in MCTS and subsequently study its impact on the algorithm's performance. In particular, TDL is used to learn a linear function approximator that is used as an a priori bias to the move selection in the algorithm's default policy; the function approximator is also used to bias the values of the nodes in the tree directly. The goal of this work is to determine whether such a simplistic approach can be used to improve the performance of MCTS for the well-known board game Othello. The analysis of the results highlights the broader conclusions that may be drawn with respect to non-random default policies in general.</td>
</tr>
<tr id="bib_Robles2011Learning" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Robles2011Learning,
  author = {Robles, David and Rohlfshagen, Philipp and Lucas, Simon M},
  title = {Learning Non-Random Moves for Playing Othello: Improving Monte Carlo Tree Search},
  booktitle = {Proc. IEEE Conference on Computational Intelligence in Games},
  year = {2011},
  pages = {305--312}
}
</pre></td>
</tr>
<tr id="Rohanimanesh2001Decision" class="entry">
	<td>Rohanimanesh, K. and Mahadevan, S.</td>
	<td>Decision-theoretic Planning with Concurrent Temporally Extended Actions <p class="infolinks">[<a href="javascript:toggleInfo('Rohanimanesh2001Decision','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Rohanimanesh2001Decision','bibtex')">BibTeX</a>]</p></td>
	<td>2001</td>
	<td>Proc AUAI Conference on Uncertainty in Artificial Intelligence, pp. 472-479&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Rohanimanesh2001Decision" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We investigate a model for planning under uncertainty with temporallyextended actions, where multiple actions can be taken concurrently at each decision epoch. Our model is based on the options framework, and combines it with factored state space models,where the set of options can be partitioned into classes that affectdisjoint state variables. We show that the set of decisionepochs for concurrent options defines a semi-Markov decisionprocess, if the underlying temporally extended actions being parallelized arerestricted to Markov options. This property allows us to use SMDPalgorithms for computing the value function over concurrentoptions. The concurrent options model allows overlapping execution ofoptions in order to achieve higher performance or in order to performa complex task. We describe a simple experiment using a navigationtask which illustrates how concurrent options results in a faster planwhen compared to the case when only one option is taken at a time.</td>
</tr>
<tr id="bib_Rohanimanesh2001Decision" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Rohanimanesh2001Decision,
  author = {Rohanimanesh, Khashayar and Mahadevan, Sridhar},
  title = {Decision-theoretic Planning with Concurrent Temporally Extended Actions},
  booktitle = {Proc AUAI Conference on Uncertainty in Artificial Intelligence},
  year = {2001},
  pages = {472--479}
}
</pre></td>
</tr>
<tr id="Russell2003Artificial" class="entry">
	<td>Russell, S.J. and Norvig, P.</td>
	<td>Artificial Intelligence: A Modern Approach <p class="infolinks" [<a href="javascript:toggleInfo('Russell2003Artificial','bibtex')">BibTeX</a>]</p></td>
	<td>2003</td>
	<td>&nbsp;</td>
	<td>book</td>
</tr>
<tr id="bib_Russell2003Artificial" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@book{Russell2003Artificial,
  author = {Russell, Stuart J. and Norvig, Peter},
  title = {Artificial Intelligence: A Modern Approach},
  publisher = {Pearson Education},
  year = {2003},
  edition = {2}
}
</pre></td>
</tr>
<tr id="Samothrakis2011Fast" class="entry">
	<td>Samothrakis, S., Robles, D. and Lucas, S.</td>
	<td>Fast Approximate Max-n Monte Carlo Tree Search for Ms Pac-Man <p class="infolinks">[<a href="javascript:toggleInfo('Samothrakis2011Fast','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Samothrakis2011Fast','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>IEEE Trans. on Computational Intelligence and AI in Games (CAIG'11)<br/>Vol. 3(2), pp. 142-154&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Samothrakis2011Fast" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present an application of Monte Carlo tree search (MCTS) for the game of Ms Pac-Man. Contrary to most applications of MCTS to date, Ms Pac-Man requires almost real-time decision making and does not have a natural end state. We approached the problem by performing Monte Carlo tree searches on a five player maxn tree representation of the game with limited tree search depth. We performed a number of experiments using both the MCTS game agents (for pacman and ghosts) and agents used in previous work (for ghosts). Performance-wise, our approach gets excellent scores, outperforming previous non-MCTS opponent approaches to the game by up to two orders of magnitude.</td>
</tr>
<tr id="bib_Samothrakis2011Fast" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Samothrakis2011Fast,
  author = {Samothrakis, S. and Robles, D. and Lucas, S.},
  title = {Fast Approximate Max-n Monte Carlo Tree Search for Ms Pac-Man},
  journal = {IEEE Trans. on Computational Intelligence and AI in Games (CAIG'11)},
  year = {2011},
  volume = {3},
  number = {2},
  pages = {142-154}
}
</pre></td>
</tr>
<tr id="Sandholm2015Abstraction" class="entry">
	<td>Sandholm, T.</td>
	<td>Abstraction for Solving Large Incomplete-Information Games <p class="infolinks">[<a href="javascript:toggleInfo('Sandholm2015Abstraction','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sandholm2015Abstraction','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proc. AAAI Conference on Artificial Intelligence&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Sandholm2015Abstraction" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Most real-world games and many recreational games are games of incomplete information. Over the last dozen years, abstraction has emerged as a key enabler for solving large incomplete-information games. First, the game is abstracted to generate a smaller, abstract game that is strategically similar to the original game. Second, an approximate equilibrium is computed in the abstract game. Third, the strategy from the abstract game is mapped back to the original game.<p>In this paper, I will review key developments in the field. I present reasons for abstracting games, and point out the issue of abstraction pathology. I then review the practical algorithms for information abstraction and action abstraction. I then cover recent theoretical breakthroughs that beget bounds on the quality of the strategy from the abstract game, when measured in the original game. I then discuss how to reverse map the opponent’s action into the abstraction if the opponent makes a move that is not in the abstraction. Finally, I discuss other topics of current and future research.</td>
</tr>
<tr id="bib_Sandholm2015Abstraction" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Sandholm2015Abstraction,
  author = {Sandholm, Tuomas},
  title = {Abstraction for Solving Large Incomplete-Information Games},
  booktitle = {Proc. AAAI Conference on Artificial Intelligence},
  year = {2015}
}
</pre></td>
</tr>
<tr id="Schaefer2008UCT" class="entry">
	<td>Sch&auml;fer, J.</td>
	<td>The UCT Algorithm Applied to Games with Imperfect Information <p class="infolinks">[<a href="javascript:toggleInfo('Schaefer2008UCT','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Schaefer2008UCT','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td><i>School</i>: Otto-von-Guericke-Universit&auml;t Magdeburg&nbsp;</td>
	<td>mastersthesis</td>
</tr>
<tr id="abs_Schaefer2008UCT" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The UCT algorithm was already used to search the best strategy in games with perfect information. The algorithm is known to handle games with high branching factors in the search tree very well. For the game of Go a computer player was implemented that uses the UCT algorithm and was able to cope with the best computer Go players. For games with imperfect information, this algorithm is now used for the first time. For the card game of Skat an implementation of the UCT algorithm is available that uses the algorithm in all three phases of the Skat game. These phases are bidding, discarding and playing the tricks. To be able to use the algorithm all unknown information must be replaced by simulated information. After that, the UCT algorithm can be used as in a game with perfect information. All results of the simulations are saved in one search tree with information sets.<p>The playing strength of the UCT player is tested against other computer players and against human players. For this purpose a tournament simulation program for Skat was developed. With this simulation program it is possible to let AI players compete against other AI players in games with predefined card distributions. Every player is given then the same chances to make all games. In games against human players a normal Skat series is simulated.<p>The UCT player can keep up with other computer players. Although it might not have reached its highest playing strength. The best combination of parameters is still to be found. The number of simulations the player can compute in a reasonable time for each move decision is also too low. The presented implementation is not optimized for speed, yet. Higher numbers of simulations will result in a higher playing strength. The UCT player can also keep up with human players. It looses the Skat series only due to conservative bidding.</td>
</tr>
<tr id="bib_Schaefer2008UCT" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@mastersthesis{Schaefer2008UCT,
  author = {Sch&auml;fer, Jan},
  title = {The UCT Algorithm Applied to Games with Imperfect Information},
  school = {Otto-von-Guericke-Universit&auml;t Magdeburg},
  year = {2008}
}
</pre></td>
</tr>
<tr id="Sephton2014Heuristic" class="entry">
	<td>Sephton, N., Cowling, P., Powley, E., Slaven, N.H. and others</td>
	<td>Heuristic move pruning in Monte Carlo Tree Search for the strategic card game Lords of War <p class="infolinks">[<a href="javascript:toggleInfo('Sephton2014Heuristic','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sephton2014Heuristic','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Proc. IEEE Conference on Computational Intelligence and Games, pp. 1-7&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Sephton2014Heuristic" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Move pruning is a technique used in game tree search which incorporates heuristic knowledge to reduce the number of moves under consideration from a particular game state. This paper investigates Heuristic Move Pruning on the strategic card game Lords of War. We use heuristics to guide our pruning and experiment with different techniques of applying pruning and their relative effectiveness. We also present a technique of artificially rolling forward a game state in an attempt to more accurately determine which moves are appropriate to prune from the decision tree. We demonstrate that heuristic move pruning is effective in Lords of War, and also that artificially rolling forward the game state can increase the effectiveness of heuristic move pruning.</td>
</tr>
<tr id="bib_Sephton2014Heuristic" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Sephton2014Heuristic,
  author = {Sephton, Nick and Cowling, Peter and Powley, Edward and Slaven, Nicholas H and others},
  title = {Heuristic move pruning in Monte Carlo Tree Search for the strategic card game Lords of War},
  booktitle = {Proc. IEEE Conference on Computational Intelligence and Games},
  year = {2014},
  pages = {1--7}
}
</pre></td>
</tr>
<tr id="Shen2006Automatic" class="entry">
	<td>Shen, J., Gu, G. and Liu, H.</td>
	<td>Automatic option generation in hierarchical reinforcement learning via immune clustering <p class="infolinks">[<a href="javascript:toggleInfo('Shen2006Automatic','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Shen2006Automatic','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>Proc. Symposium on Systems and Control in Aerospace and Astronautics (ISSCAA'06), pp. 4-pp&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Shen2006Automatic" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: An open problem in hierarchical reinforcement learning is how to automatically generate hierarchies, e.g. options. We consider an immune clustering approach for automatic construction of options in a dynamic environment. The learning agent generates an undirected edge-weighted topological graph of the environment state transitions online. An immune clustering algorithm is then used to partition the state space. A second immune response algorithm is used to update the clusters when a new state being encountered later. Local strategies for reaching the different parts of the space are separately learned and added to the model in a form of options. By our approach, the options not only can be automatically generated but also can be dynamically updated</td>
</tr>
<tr id="bib_Shen2006Automatic" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Shen2006Automatic,
  author = {Shen, Jing and Gu, Guochang and Liu, Haibo},
  title = {Automatic option generation in hierarchical reinforcement learning via immune clustering},
  booktitle = {Proc. Symposium on Systems and Control in Aerospace and Astronautics (ISSCAA'06)},
  year = {2006},
  pages = {4--pp}
}
</pre></td>
</tr>
<tr id="Silver2012Temporal" class="entry">
	<td>Silver, D., Sutton, R.S. and M&uuml;ller, M.</td>
	<td>Temporal-Difference Search in Computer Go <p class="infolinks">[<a href="javascript:toggleInfo('Silver2012Temporal','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Silver2012Temporal','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Machine Learning<br/>Vol. 87(2), pp. 183-219&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Silver2012Temporal" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Temporal-difference learning is one of the most successful and broadly applied solutions to the reinforcement learning problem; it has been used to achieve master-level play in chess, checkers and backgammon. The key idea is to update a value function from episodes of real experience, by bootstrapping from future value estimates, and using value function approximation to generalise between related states. Monte-Carlo tree search is a recent algorithm for high-performance search, which has been used to achieve master-level play in Go. The key idea is to use the mean outcome of simulated episodes of experience to evaluate each state in a search tree. We introduce a new approach to high-performance search in Markov decision processes and two-player games. Our method, temporal-difference search, combines temporal-difference learning with simulation-based search. Like Monte-Carlo tree search, the value function is updated from simulated experience; but like temporal-difference learning, it uses value function approximation and bootstrapping to efficiently generalise between related states. We apply temporal-difference search to the game of 9×9 Go, using a million binary features matching simple patterns of stones. Without any explicit search tree, our approach outperformed an unenhanced Monte-Carlo tree search with the same number of simulations. When combined with a simple alpha-beta search, our program also outperformed all traditional (pre-Monte-Carlo) search and machine learning programs on the 9×9 Computer Go Server.</td>
</tr>
<tr id="bib_Silver2012Temporal" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Silver2012Temporal,
  author = {Silver, David and Sutton, Richard S and M&uuml;ller, Martin},
  title = {Temporal-Difference Search in Computer Go},
  journal = {Machine Learning},
  publisher = {Springer},
  year = {2012},
  volume = {87},
  number = {2},
  pages = {183--219}
}
</pre></td>
</tr>
<tr id="Silver2010Monte" class="entry">
	<td>Silver, D. and Veness, J.</td>
	<td>Monte-Carlo Planning in Large POMDPs <p class="infolinks">[<a href="javascript:toggleInfo('Silver2010Monte','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Silver2010Monte','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>Advances in Neural Information Processing Systems, pp. 2164-2172&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Silver2010Monte" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent's belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, Monte- Carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simu- lator of the POMDP is required, rather than explicit probability distribu- tions. These properties enable POMCP to plan effectively in significantly larger POMDPs than has previously been possible. We demonstrate its ef- fectiveness in three large POMDPs. We scale up a well-known benchmark problem, rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10 Ã 10 battleship and partially observable PacMan, with approximately 1018 and 1056 states respectively. Our Monte- Carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the first general purpose planner to achieve high performance in such large and unfactored POMDPs.</td>
</tr>
<tr id="bib_Silver2010Monte" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Silver2010Monte,
  author = {Silver, David and Veness, Joel},
  title = {Monte-Carlo Planning in Large POMDPs},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2010},
  pages = {2164--2172}
}
</pre></td>
</tr>
<tr id="Singh1992Reinforcement" class="entry">
	<td>Singh, S.P.</td>
	<td>Reinforcement Learning with a Hierarchy of Abstract Models <p class="infolinks">[<a href="javascript:toggleInfo('Singh1992Reinforcement','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Singh1992Reinforcement','bibtex')">BibTeX</a>]</p></td>
	<td>1992</td>
	<td>Proc. AAAI Conference on Artificial Intelligence, pp. 202-207&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Singh1992Reinforcement" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement learning (RL) algorithms have traditionally been thought of as trial and error learning methods that use actual control experience to incrementally improve a control policy. Sutton's DYNA architecture demonstrated that RL algorithms can work as well using simulated experience from an environment model, and that the resulting computation was similar to doing one-step lookahead planning. Inspired by the literature on hierarchical planning, I propose learning a hierarchy of models of the environment that abstract temporal detail as a means of improving the scalability of RL algorithms. I present H-DYNA (Hierarchical DYNA), an extension to Sutton's DYNA architecture that is able to learn such a hierarchy of abstract models. H-DYNA differs from hierarchical planners in two ways: first, the abstract models are learned using experience gained while learning to solve other tasks in the same environment, and second, the abstract models can be used to solve stochastic control tasks. Simulations on a set of compositionally-structured navigation tasks show that H-DYNA can learn to solve them faster than conventional RL algorithms. The abstract models also serve as mechanisms for achieving transfer of learning across multiple tasks.</td>
</tr>
<tr id="bib_Singh1992Reinforcement" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Singh1992Reinforcement,
  author = {Singh, Satinder P},
  title = {Reinforcement Learning with a Hierarchy of Abstract Models},
  booktitle = {Proc. AAAI Conference on Artificial Intelligence},
  year = {1992},
  pages = {202--207}
}
</pre></td>
</tr>
<tr id="Singh1995Reinforcement" class="entry">
	<td>Singh, S.P., Jaakkola, T. and Jordan, M.I.</td>
	<td>Reinforcement Learning with Soft State Aggregation <p class="infolinks">[<a href="javascript:toggleInfo('Singh1995Reinforcement','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Singh1995Reinforcement','bibtex')">BibTeX</a>]</p></td>
	<td>1995</td>
	<td>Proc. Conference on Advances in Neural Information Processing Systems, pp. 361-368&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Singh1995Reinforcement" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: It is widely accepted that the use of more compact representations than lookup tables is crucial to scaling reinforcement learning (RL) algorithms to real-world problems. Unfortunately almost all of the theory of reinforcement learning assumes lookup table representations. In this paper we address the pressing issue of combining function approximation and RL, and present 1) a function approximator based on a simple extension to state aggregation (a commonly used form of compact representation), namely soft state aggregation, 2) a theory of convergence for RL with arbitrary, but fixed, soft state aggregation, 3) a novel intuitive understanding of the effect of state aggregation on online RL, and 4) a new heuristic adaptive state aggregation algorithm that finds improved compact representations by exploiting the non-discrete nature of soft state aggregation. Preliminary empirical results are also presented.</td>
</tr>
<tr id="bib_Singh1995Reinforcement" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Singh1995Reinforcement,
  author = {Satinder P. Singh and Tommi Jaakkola and Michael I. Jordan},
  title = {Reinforcement Learning with Soft State Aggregation},
  booktitle = {Proc. Conference on Advances in Neural Information Processing Systems},
  year = {1995},
  pages = {361--368}
}
</pre></td>
</tr>
<tr id="Spronck2010Player" class="entry">
	<td>Spronck, P. and den Teuling, F.</td>
	<td>Player Modeling in Civilization IV <p class="infolinks">[<a href="javascript:toggleInfo('Spronck2010Player','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Spronck2010Player','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>Proc. AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Spronck2010Player" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This research aims at building a preference-based player model of Civilization IV players. Our model incorporates attributes which are defined for AI players. We use a sequential minimal optimization (SMO) classifier to build the player model based on a training set with observations of a large number of games between six AI players. The model was validated on a test set of games between the same six AI players. While it did not seem to generalize well to the preferences of different AI players, it did manage to accurately predict some of the preferences for a veteran human player. Further tests showed that AI players with the same play styles but different preference values were often confused by the model. We conclude that for a complex game such as Civilization IV a model that attempts to accurately predict specific preference values is hard to construct. A model that focusses on play styles might succeed better.</td>
</tr>
<tr id="bib_Spronck2010Player" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Spronck2010Player,
  author = {Spronck, Pieter and den Teuling, Freek},
  title = {Player Modeling in Civilization IV},
  booktitle = {Proc. AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  publisher = {The AAAI Press},
  year = {2010}
}
</pre></td>
</tr>
<tr id="Stolle2002Learning" class="entry">
	<td>Stolle, M. and Precup, D.</td>
	<td>Learning options in reinforcement learning <p class="infolinks">[<a href="javascript:toggleInfo('Stolle2002Learning','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Stolle2002Learning','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td>Abstraction, Reformulation, and Approximation, pp. 212-223&nbsp;</td>
	<td>incollection</td>
</tr>
<tr id="abs_Stolle2002Learning" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Temporally extended actions (e.g., macro actions) have proven very useful in speeding up learning, ensuring robustness and building prior knowledge into AI systems. The options framework (Precup, 2000; Sutton, Precup &amp; Singh, 1999) provides a natural way of incorporating such actions into reinforcement learning systems, but leaves open the issue of how good options might be identi- fied. In this paper, we empirically explore a simple approach to creating options. The underlying assumption is that the agent will be asked to perform different goal-achievement tasks in an environment that is otherwise the same over time. Our approach is based on the intuition that “bottleneck” states, i.e. states that are frequently visited on system trajectories, could prove to be useful subgoals (e.g. McGovern &amp; Barto, 2001; Iba, 1989).<p>We present empirical studies of this approach in two gridworld navigation tasks. One of the environments we explored contains bottleneck states, and the algorithm indeed finds these states, as expected. The second environment is an empty gridworld with no obstacles. Although the environment does not contain bottleneck states, our approach still finds useful options, which essentially allow the agent to travel around the environment more quickly.</td>
</tr>
<tr id="bib_Stolle2002Learning" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Stolle2002Learning,
  author = {Stolle, Martin and Precup, Doina},
  title = {Learning options in reinforcement learning},
  booktitle = {Abstraction, Reformulation, and Approximation},
  publisher = {Springer},
  year = {2002},
  pages = {212--223}
}
</pre></td>
</tr>
<tr id="Sutton1992Reinforcement" class="entry">
	<td>Sutton, R.S.</td>
	<td>Reinforcement Learning Architectures <p class="infolinks">[<a href="javascript:toggleInfo('Sutton1992Reinforcement','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sutton1992Reinforcement','bibtex')">BibTeX</a>]</p></td>
	<td>1992</td>
	<td>Proc. International Symposium on Neural Information Processing&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Sutton1992Reinforcement" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement learning is the learning of a mapping from situations to actions so as to maximize a scalar reward or reinforcement signal. The learner is not told which action to take, as in most forms of learning, but instead must discover which actions yield the highest reward by trying them. In the most interesting and challenging cases, actions affect not only the immediate reward, but also the next situation, and through that all subsequent rewards. These two characteristics -- trial-and-error search and delayed reward -- are the two most important distinguishing features of reinforcement learning. In this paper I present a brief overview of the development of reinforcement learning architectures over the past decade, including reinforcement-comparison, actor-critic, and Q-learning architectures. Finally, I present Dyna, a class of architectures based on reinforcement learning but which go beyond trial-and-error learning to include a learned internal model of the world.</td>
</tr>
<tr id="bib_Sutton1992Reinforcement" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Sutton1992Reinforcement,
  author = {Sutton, Richard S.},
  title = {Reinforcement Learning Architectures},
  booktitle = {Proc. International Symposium on Neural Information Processing},
  year = {1992}
}
</pre></td>
</tr>
<tr id="Sutton1999Between" class="entry">
	<td>Sutton, R.S., Precup, D. and Singh, S.</td>
	<td>Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning <p class="infolinks">[<a href="javascript:toggleInfo('Sutton1999Between','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sutton1999Between','bibtex')">BibTeX</a>]</p></td>
	<td>1999</td>
	<td>Artificial intelligence<br/>Vol. 112(1), pp. 181-211&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Sutton1999Between" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.</td>
</tr>
<tr id="bib_Sutton1999Between" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Sutton1999Between,
  author = {Sutton, Richard S and Precup, Doina and Singh, Satinder},
  title = {Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  journal = {Artificial intelligence},
  publisher = {Elsevier},
  year = {1999},
  volume = {112},
  number = {1},
  pages = {181--211}
}
</pre></td>
</tr>
<tr id="Szita2010Monte" class="entry">
	<td>Szita, I., Chaslot, G. and Spronck, P.</td>
	<td>Monte-carlo tree search in Settlers of Catan <p class="infolinks">[<a href="javascript:toggleInfo('Szita2010Monte','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Szita2010Monte','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>Advances in Computer Games, pp. 21-32&nbsp;</td>
	<td>incollection</td>
</tr>
<tr id="abs_Szita2010Monte" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Games are considered important benchmark opportunities for artificial intelligence research. Modern strategic board games can typically be played by three or more people, which makes them suit- able test beds for investigating multi-player strategic decision making. Monte-Carlo Tree Search (MCTS) is a recently published family of algorithms that achieved successful results  ith classical, two-player, perfect-information games such as Go. In this paper we apply MCTS to the multi-player, non-deterministic board game Settlers of Catan. We implemented an agent that is able to play  gainst computer-controlled and human players. We show that MCTS can be adapted successfully to multi-agent environments, and present two approaches of providing the agent with a limited amount of domain knowledge. Our results show that the agent has a considerable playing strength when compared to game implementation with existing heuristics. So, we may conclude that MCTS is a suitable tool for achieving a strong Settlers of Catan player.</td>
</tr>
<tr id="bib_Szita2010Monte" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Szita2010Monte,
  author = {Szita, Istv&aacute;n and Chaslot, Guillaume and Spronck, Pieter},
  title = {Monte-carlo tree search in Settlers of Catan},
  booktitle = {Advances in Computer Games},
  publisher = {Springer},
  year = {2010},
  pages = {21--32}
}
</pre></td>
</tr>
<tr id="Tadepalli2004Relational" class="entry">
	<td>Tadepalli, P., Givan, R. and Driessens, K.</td>
	<td>Relational reinforcement learning: An overview <p class="infolinks">[<a href="javascript:toggleInfo('Tadepalli2004Relational','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Tadepalli2004Relational','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td>Proceedings of the ICML-2004 Workshop on Relational Reinforcement Learning, pp. 1-9&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Tadepalli2004Relational" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Relational Reinforcement Learning (RRL) is both a young and an old field. In this paper, we trace the history of the field to related disciplines, outline some current work and promising new directions, and survey the research issues and opportunities that lie ahead.</td>
</tr>
<tr id="bib_Tadepalli2004Relational" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Tadepalli2004Relational,
  author = {Tadepalli, Prasad and Givan, Robert and Driessens, Kurt},
  title = {Relational reinforcement learning: An overview},
  booktitle = {Proceedings of the ICML-2004 Workshop on Relational Reinforcement Learning},
  year = {2004},
  pages = {1--9}
}
</pre></td>
</tr>
<tr id="Tak2014Monte" class="entry">
	<td>Tak, M.J., Lanctot, M. and Winands, M.H.</td>
	<td>Monte Carlo Tree Search variants for simultaneous move games <p class="infolinks">[<a href="javascript:toggleInfo('Tak2014Monte','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Tak2014Monte','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Proc. IEEE Conference on Computational Intelligence and Games, pp. 1-8&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Tak2014Monte" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Monte Carlo Tree Search (MCTS) is a widely-used technique for game-tree search in sequential turn-based games. The extension to simultaneous move games, where all players choose moves simultaneously each turn, is non-trivial due to the complexity of this class of games. In this paper, we describe simultaneous move MCTS and analyze its application in a set of nine disparate simultaneous move games. We use several possible variants, Decoupled UCT, Sequential UCT, Exp3, and Regret Matching. These variants include both deterministic and stochastic selection strategies and we characterize the game-play performance of each one. The results indicate that the relative performance of each variant depends strongly on the game and the opponent, and that parameter tuning can also not be as straightforward as the purely sequential case. Overall, Decoupled UCT performs best despite its theoretical shortcomings.</td>
</tr>
<tr id="bib_Tak2014Monte" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Tak2014Monte,
  author = {Tak, Mandy JW and Lanctot, Marc and Winands, Mark HM},
  title = {Monte Carlo Tree Search variants for simultaneous move games},
  booktitle = {Proc. IEEE Conference on Computational Intelligence and Games},
  year = {2014},
  pages = {1--8}
}
</pre></td>
</tr>
<tr id="Thrun1995Finding" class="entry">
	<td>Thrun, S. and Schwartz, A.</td>
	<td>Finding Structure in Reinforcement Learning <p class="infolinks">[<a href="javascript:toggleInfo('Thrun1995Finding','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Thrun1995Finding','bibtex')">BibTeX</a>]</p></td>
	<td>1995</td>
	<td>Proc. Conference on Advances in Neural Information Processing Systems, pp. 385-392&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Thrun1995Finding" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement learning addresses the problem of learning to select actions in order to maximize one’s performance in unknown environments. To scale reinforcement learning to complex real-world tasks, such as typically studied in AI, one must ultimately be able to discover the structure in the world, in order to abstract away the myriad of details and to operate in more tractable problem spaces.<p>This paper presents the SKILLS algorithm. SKILLS discovers skills, which are partially defined action policies that arise in the context of multiple, related tasks. Skills collapse whole action sequences into single operators. They are learned by minimizing the compactness of action policies, using a description length argument on their representation. Empirical results in simple grid navigation tasks illustrate the successful discovery of structure in reinforcement learning.</td>
</tr>
<tr id="bib_Thrun1995Finding" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Thrun1995Finding,
  author = {Sebastian Thrun and Anton Schwartz},
  title = {Finding Structure in Reinforcement Learning},
  booktitle = {Proc. Conference on Advances in Neural Information Processing Systems},
  publisher = {MIT Press},
  year = {1995},
  pages = {385--392}
}
</pre></td>
</tr>
<tr id="Tokadli2014Option" class="entry">
	<td>Tokadli, G. and Feight, K.M.</td>
	<td>Option and Constraint Generation using Work Domain Analysis <p class="infolinks">[<a href="javascript:toggleInfo('Tokadli2014Option','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Tokadli2014Option','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Proc. IEEE Conference on System, Man, and Cybernetics&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Tokadli2014Option" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we investigate the use of Work Domain Analysis (WDA), a technique from the field of cognitive engineering, to inform the creation of options and constraints for Reinforcement Learning (RL) algorithms. The micro-world of Pac-Man, a classic arcade game, is used as a tractable and representative work domain. WDA was conducted on individuals familiar with Pac-Man and an Abstraction Hierarchy (AH), a means-ends representation of their understanding of the game, was created for each individual. The abstraction hierarchies for best performing and worst performing individuals were then combined to illustrate the differences between the different groups. Several differences between the two groups were found, and included the use of defense as well as offensive strategies by high performers versus only defense by poor performers, context sensitivity and additional goals and more sophisticated constraints by high performers. The differences were translated into an options and constraint paradigm suitable for incorporation into RL algorithms.</td>
</tr>
<tr id="bib_Tokadli2014Option" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Tokadli2014Option,
  author = {Tokadli, G and Feight, K. M.},
  title = {Option and Constraint Generation using Work Domain Analysis},
  booktitle = {Proc. IEEE Conference on System, Man, and Cybernetics},
  year = {2014}
}
</pre></td>
</tr>
<tr id="Uriarte2014Game" class="entry">
	<td>Uriarte, A. and Onta n&oacute;n, S.</td>
	<td>Game-Tree Search over High-Level Game States in RTS Games <p class="infolinks">[<a href="javascript:toggleInfo('Uriarte2014Game','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Uriarte2014Game','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Tenth Artificial Intelligence and Interactive Digital Entertainment ConferenceProc. AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Uriarte2014Game" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: From an AI point of view, Real-Time Strategy (RTS) games are hard because they have enormous state spaces, they are real-time and partially observable. In this paper, we present an approach to deploy game-tree search in RTS games by using game state abstraction. We propose a high-level abstract representation of the game state, that significantly reduces the branching factor when used for game-tree search algorithms. Using this high-level representation, we evaluate versions of alpha-beta search and of Monte Carlo Tree Search (MCTS). We present experiments in the context of StarCraft showing promising results in dealing with the large branching factors present in RTS games.</td>
</tr>
<tr id="bib_Uriarte2014Game" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Uriarte2014Game,
  author = {Uriarte, Alberto and Onta&ntilde;&oacute;n, Santiago},
  title = {Game-Tree Search over High-Level Game States in RTS Games},
  booktitle = {Proc. AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  journal = {Tenth Artificial Intelligence and Interactive Digital Entertainment Conference},
  year = {2014}
}
</pre></td>
</tr>
<tr id="Uriarte2014High" class="entry">
	<td>Uriarte, A. and Onta n&oacute;n, S.</td>
	<td>High-Level Representations for Game-Tree Search in RTS Games <p class="infolinks">[<a href="javascript:toggleInfo('Uriarte2014High','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Uriarte2014High','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Proc. AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Uriarte2014High" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: From an AI point of view, Real-Time Strategy (RTS) games are hard because they have enormous state spaces, they are real-time and partially observable. In this paper, we explore an approach to deploy gametree search in RTS games by using game state abstraction, and explore the effect of using different abstractions over the game state. Different abstractions capture different parts of the game state, and result in different branching factors when used for game-tree search algorithms. We evaluate the different representations using Monte Carlo Tree Search in the context of StarCraft.</td>
</tr>
<tr id="bib_Uriarte2014High" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Uriarte2014High,
  author = {Uriarte, Alberto and Onta&ntilde;&oacute;n, Santiago},
  title = {High-Level Representations for Game-Tree Search in RTS Games},
  booktitle = {Proc. AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  year = {2014}
}
</pre></td>
</tr>
<tr id="Vien2015Hierarchical" class="entry">
	<td>Vien, N.A. and Toussaint, M.</td>
	<td>Hierarchical Monte-Carlo Planning <p class="infolinks">[<a href="javascript:toggleInfo('Vien2015Hierarchical','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Vien2015Hierarchical','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proc. AAAI Conference on Artificial Intelligence&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Vien2015Hierarchical" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Monte-Carlo Tree Search, especially UCT and its POMDP version POMCP, have demonstrated excellent performance on many problems. However, to efficiently scale to large domains one should also exploit hierarchical structure if present. In such hierarchical domains, finding rewarded states typically requires to search deeply; covering enough such informative states very far from the root becomes computationally expensive in flat non-hierarchical search approaches. We propose novel, scalable MCTS methods which integrate a task hierarchy into the MCTS framework, specifically leading to hierarchical versions of both, UCT and POMCP. The new method does not need to estimate probabilistic models of each subtask, it instead computes subtask policies purely sample-based. We evaluate the hierarchical MCTS methods on various settings such as a hierarchical MDP, a Bayesian model-based hierarchical RL problem, and a large hierarchical POMDP.</td>
</tr>
<tr id="bib_Vien2015Hierarchical" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Vien2015Hierarchical,
  author = {Vien, Ngo Anh and Toussaint, Marc},
  title = {Hierarchical Monte-Carlo Planning},
  booktitle = {Proc. AAAI Conference on Artificial Intelligence},
  year = {2015}
}
</pre></td>
</tr>
<tr id="Ward2009Monte" class="entry">
	<td>Ward, C. and Cowling, P.</td>
	<td>Monte Carlo Search Applied to Card Selection in Magic: The Gathering <p class="infolinks">[<a href="javascript:toggleInfo('Ward2009Monte','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ward2009Monte','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td>Proc. IEEE Conference on Computational Intelligence in Games, pp. 9-16&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Ward2009Monte" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present the card game Magic: the Gathering as an interesting test bed for AI research. We believe that the complexity of the game offers new challenges in areas such as search in imperfect information domains and opponent modelling. Since there are a thousands of possible cards, and many cards change the rules to some extent, to successfully build AI for magic: the gathering ultimately requires a rather general form of game intelligence (although we only consider a small subset of these cards in this paper). We create a range of players based on stochastic, rule-based and Monte Carlo approaches and investigate Monte Carlo search with and without the use of a sophisticated rule-based approach to generate game rollouts. We also examine the effect of increasing numbers of Monte Carlo simulations on playing strength and investigate whether Monte Carlo simulations can enable an otherwise weak player to overcome a stronger rule-based player. Overall, we show that Monte Carlo search is a promising avenue for generating a strong AI player for magic: the gathering.</td>
</tr>
<tr id="bib_Ward2009Monte" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Ward2009Monte,
  author = {Ward, C.D. and Cowling, P.I},
  title = {Monte Carlo Search Applied to Card Selection in Magic: The Gathering},
  booktitle = {Proc. IEEE Conference on Computational Intelligence in Games},
  year = {2009},
  pages = {9-16}
}
</pre></td>
</tr>
<tr id="Wender2008Using" class="entry">
	<td>Wender, S. and Watson, I.</td>
	<td>Using Reinforcement Learning for City Site Selection in the Turn-Based Strategy Game Civilization IV <p class="infolinks">[<a href="javascript:toggleInfo('Wender2008Using','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wender2008Using','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>Proc. IEEE Symposium on Computational Intelligence in Games, pp. 372-377&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Wender2008Using" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper describes the design and implementation of a reinforcement learner based on Q-Learning. This adaptive agent is applied to the city placement selection task in the commercial computer game Civilization IV. The city placement selection determines the founding sites for the cities in this turn-based empire building game from the Civilization series. Our aim is the creation of an adaptive machine learning approach for a task which is originally performed by a complex deterministic script. This machine learning approach results in a more challenging and dynamic computer AI. We present the preliminary findings on the performance of our reinforcement learning approach and we make a comparison between the performance of the adaptive agent and the original static game AI. Both the comparison and the performance measurements show encouraging results. Furthermore the behaviour and performance of the learning algorithm are elaborated and ways of extending our work are discussed.</td>
</tr>
<tr id="bib_Wender2008Using" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Wender2008Using,
  author = {Wender, S. and Watson, I},
  title = {Using Reinforcement Learning for City Site Selection in the Turn-Based Strategy Game Civilization IV},
  booktitle = {Proc. IEEE Symposium on Computational Intelligence in Games},
  year = {2008},
  pages = {372-377}
}
</pre></td>
</tr>
<tr id="Whitehouse2014Monte" class="entry">
	<td>Whitehouse, D.</td>
	<td>Monte Carlo Tree Search for games with Hidden Information and Unvertainty <p class="infolinks">[<a href="javascript:toggleInfo('Whitehouse2014Monte','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Whitehouse2014Monte','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td><i>School</i>: University of York&nbsp;</td>
	<td>phdthesis</td>
</tr>
<tr id="abs_Whitehouse2014Monte" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Monte Carlo Tree Search (MCTS) is an AI technique that has been successfully applied to many deterministic games of perfect information, leading to large advances in a number of domains, such as Go and General Game Playing. Imperfect information games are less well studied in the field of AI despite being popular and of significant commercial interest, for example in the case of computer and mobile adaptations of turn based board and card games. This is largely because hidden information and uncertainty leads to a large increase in complexity compared to perfect information games.<p>In this thesis MCTS is extended to games with hidden information and uncertainty through the introduction of the Information Set MCTS (ISMCTS) family of algorithms. It is demonstrated that ISMCTS can handle hidden information and uncertainty in a variety of complex board and card games. This is achieved whilst preserving the general applicability of MCTS and using computational budgets appropriate for use in a commercial game. The ISMCTS algorithm is shown to outperform the existing approach of Perfect Information Monte Carlo (PIMC) search. Additionally it is shown that ISMCTS can be used to solve two known issues with PIMC search, namely strategy fusion and non-locality. ISMCTS has been integrated into a commercial game, Spades by AI Factory, with over 2.5 million downloads.<p>The Information Capture And ReUSe (ICARUS) framework is also introduced in this thesis. The ICARUS framework generalises MCTS enhancements in terms of information capture (from MCTS simulations) and reuse (to improve MCTS tree and simulation policies). The ICARUS framework is used to express existing enhancements, to provide a tool to design new ones, and to rigorously define how MCTS enhancements can be combined. The ICARUS framework is tested across a wide variety of games.</td>
</tr>
<tr id="bib_Whitehouse2014Monte" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@phdthesis{Whitehouse2014Monte,
  author = {Whitehouse, Daniel},
  title = {Monte Carlo Tree Search for games with Hidden Information and Unvertainty},
  school = {University of York},
  year = {2014}
}
</pre></td>
</tr>
<tr id="Whitehouse2011Determinization" class="entry">
	<td>Whitehouse, D., Powley, E. and Cowling, P.</td>
	<td>Determinization and Information Set Monte Carlo Tree Search for the Card Game Dou Di Zhu <p class="infolinks">[<a href="javascript:toggleInfo('Whitehouse2011Determinization','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Whitehouse2011Determinization','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Proc. IEEE Conference on Computational Intelligence and Games (CIG'11), pp. 87-94&nbsp;</td>
	<td>inproceedings</td>
</tr>
<tr id="abs_Whitehouse2011Determinization" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Determinization is a technique for making decisions in games with stochasticity and/or imperfect information by sampling instances of the equivalent deterministic game of perfect information. Monte-Carlo Tree Search (MCTS) is an AI technique that has recently proved successful in the domain of deterministic games of perfect information. This paper studies the strengths and weaknesses of determinization coupled with MCTS on a game of imperfect information, the popular Chinese card game Dou Di Zhu. We compare a “cheating” agent (with access to hidden information) to an agent using determinization with random deals. We investigate the fraction of knowledge that a non-cheating agent could possibly infer about opponents' hidden cards. Furthermore, we show that an important source of error in determinization arises since this approach searches a tree that does not truly resemble the game tree for a game with stochasticity and imperfect information. Hence we introduce a novel variant of MCTS that operates directly on trees of information sets and show that our algorithm performs well in precisely those situations where determinization using random deals performs poorly.</td>
</tr>
<tr id="bib_Whitehouse2011Determinization" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Whitehouse2011Determinization,
  author = {Whitehouse, D. and Powley, E.J. and Cowling, P.I},
  title = {Determinization and Information Set Monte Carlo Tree Search for the Card Game Dou Di Zhu},
  booktitle = {Proc. IEEE Conference on Computational Intelligence and Games (CIG'11)},
  year = {2011},
  pages = {87-94}
}
</pre></td>
</tr>
<tr id="Whiteson2010Adaptive" class="entry">
	<td>Whiteson, S., Taylor, M.E. and Stone, P.</td>
	<td>Adaptive Tile Coding for Value Function Approximation <p class="infolinks">[<a href="javascript:toggleInfo('Whiteson2010Adaptive','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Whiteson2010Adaptive','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td><br/>Vol. 291Adaptive Representations for Reinforcement Learning, pp. 65-76&nbsp;</td>
	<td>incollection</td>
</tr>
<tr id="abs_Whiteson2010Adaptive" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement learning problems are commonly tackled by estimating the optimal value function. In many real-world problems, learning this value function requires a function approximator, which maps states to values via a parameterized function. In practice, the success of function approximators depends on the ability of the human designer to select an appropriate representation for the value function. This paper presents adaptive tile coding, a novel method that automates this design process for tile coding, a popular function approximator, by beginning with a simple representation with few tiles and refining it during learning by splitting existing tiles into smaller ones. In addition to automatically discovering effective representations, this approach provides a natural way to reduce the function approximator’s level of generalization over time. Empirical results in multiple domains compare two different criteria for deciding which tiles to split and verify that adaptive tile coding can automatically discover effective representations and that its speed of learning is competitive with the best fixed representations.</td>
</tr>
<tr id="bib_Whiteson2010Adaptive" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Whiteson2010Adaptive,
  author = {Shimon Whiteson and Matthew E. Taylor and Peter Stone},
  title = {Adaptive Tile Coding for Value Function Approximation},
  booktitle = {Adaptive Representations for Reinforcement Learning},
  publisher = {Springer Berlin Heidelberg},
  year = {2010},
  volume = {291},
  pages = {65-76}
}
</pre></td>
</tr>
<tr id="Wiering2012Reinforcement" class="entry">
	<td>Wiering, M. and van Otterlo, M.</td>
	<td>Reinforcement Learning <p class="infolinks">[<a href="javascript:toggleInfo('Wiering2012Reinforcement','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wiering2012Reinforcement','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Adaptation, Learning, and Optimization<br/>Vol. 12, pp. 652&nbsp;</td>
	<td>article</td>
</tr>
<tr id="abs_Wiering2012Reinforcement" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement learning encompasses both a science of adaptive behavior of rational beings in uncertain environments and a computational methodology for finding optimal behaviors for challenging problems in control, optimization and adaptive behavior of intelligent agents. As a field, reinforcement learning has progressed tremendously in the past decade.<p>The main goal of this book is to present an up-to-date series of survey articles on the main contemporary sub-fields of reinforcement learning. This includes surveys on partially observable environments, hierarchical task decompositions, relational knowledge representation and predictive state representations. Furthermore, topics such as transfer, evolutionary methods and continuous spaces in reinforcement learning are surveyed. In addition, several chapters review reinforcement learning methods in robotics, in games, and in computational neuroscience. In total seventeen different subfields are presented by mostly young experts in those areas, and together they truly represent a state-of-the-art of current reinforcement learning research.<p>Marco Wiering works at the artificial intelligence department of the University of Groningen in the Netherlands. He has published extensively on various reinforcement learning topics. Martijn van Otterlo works in the cognitive artificial intelligence group at the Radboud University Nijmegen in The Netherlands. He has mainly focused on expressive knowledge<br>representation in reinforcement learning settings.</td>
</tr>
<tr id="bib_Wiering2012Reinforcement" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Wiering2012Reinforcement,
  author = {Wiering, Marco and van Otterlo, Martijn},
  title = {Reinforcement Learning},
  journal = {Adaptation, Learning, and Optimization},
  publisher = {Springer},
  year = {2012},
  volume = {12},
  pages = {652}
}
</pre></td>
</tr>
</tbody>
</table>
<footer>
 <small>Created by <a href="http://jabref.sourceforge.net">JabRef</a> on 30/06/2015.</small>
</footer>
<!-- file generated by JabRef -->
</body>
</html>